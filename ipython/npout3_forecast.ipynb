{
 "metadata": {
  "name": "",
  "signature": "sha256:aa73a3f3fcdc990011ffd799f249e80a181c00967f0a768b52ae25af024ee8e1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "pd.set_option(\"display.max_columns\", 300)\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Data Initialization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv('../../MB_PCUST_NPOUT_TRAIN.txt', encoding='utf-8', header=1, sep='\\t', names=['C'+str(i+1) for i in xrange(215)], na_values=['?', '\\N'])\n",
      "\n",
      "#df = pd.read_csv('../../MB_PCUST_NPOUT_EXAM.txt', encoding='utf-8', sep='\\t', names=['C2']+['C'+str(i+4) for i in xrange(212)], na_values=['?', '\\N'])\n",
      "#df['C3'] = df['C24'].map(lambda x: 'Y' if x>0 else 'N')\n",
      "#del df['C2']\n",
      "\n",
      "#df = df.loc[df['C1']==7]\n",
      "#df = pd.concat([df.loc[df['C3']=='N'].sample(20000*3/4), df.loc[df['C3']=='Y'].sample(20000*1/4)])\n",
      "df = df.sample(10000)\n",
      "del df['C1']\n",
      "del df['C2']\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>C3</th>\n",
        "      <th>C4</th>\n",
        "      <th>C5</th>\n",
        "      <th>C6</th>\n",
        "      <th>C7</th>\n",
        "      <th>C8</th>\n",
        "      <th>C9</th>\n",
        "      <th>C10</th>\n",
        "      <th>C11</th>\n",
        "      <th>C12</th>\n",
        "      <th>C13</th>\n",
        "      <th>C14</th>\n",
        "      <th>C15</th>\n",
        "      <th>C16</th>\n",
        "      <th>C17</th>\n",
        "      <th>C18</th>\n",
        "      <th>C19</th>\n",
        "      <th>C20</th>\n",
        "      <th>C21</th>\n",
        "      <th>C22</th>\n",
        "      <th>C23</th>\n",
        "      <th>C24</th>\n",
        "      <th>C25</th>\n",
        "      <th>C26</th>\n",
        "      <th>C27</th>\n",
        "      <th>C28</th>\n",
        "      <th>C29</th>\n",
        "      <th>C30</th>\n",
        "      <th>C31</th>\n",
        "      <th>C32</th>\n",
        "      <th>C33</th>\n",
        "      <th>C34</th>\n",
        "      <th>C35</th>\n",
        "      <th>C36</th>\n",
        "      <th>C37</th>\n",
        "      <th>C38</th>\n",
        "      <th>C39</th>\n",
        "      <th>C40</th>\n",
        "      <th>C41</th>\n",
        "      <th>C42</th>\n",
        "      <th>C43</th>\n",
        "      <th>C44</th>\n",
        "      <th>C45</th>\n",
        "      <th>C46</th>\n",
        "      <th>C47</th>\n",
        "      <th>C48</th>\n",
        "      <th>C49</th>\n",
        "      <th>C50</th>\n",
        "      <th>C51</th>\n",
        "      <th>C52</th>\n",
        "      <th>C53</th>\n",
        "      <th>C54</th>\n",
        "      <th>C55</th>\n",
        "      <th>C56</th>\n",
        "      <th>C57</th>\n",
        "      <th>C58</th>\n",
        "      <th>C59</th>\n",
        "      <th>C60</th>\n",
        "      <th>C61</th>\n",
        "      <th>C62</th>\n",
        "      <th>C63</th>\n",
        "      <th>C64</th>\n",
        "      <th>C65</th>\n",
        "      <th>C66</th>\n",
        "      <th>C67</th>\n",
        "      <th>C68</th>\n",
        "      <th>C69</th>\n",
        "      <th>C70</th>\n",
        "      <th>C71</th>\n",
        "      <th>C72</th>\n",
        "      <th>C73</th>\n",
        "      <th>C74</th>\n",
        "      <th>C75</th>\n",
        "      <th>C76</th>\n",
        "      <th>C77</th>\n",
        "      <th>C78</th>\n",
        "      <th>C79</th>\n",
        "      <th>C80</th>\n",
        "      <th>C81</th>\n",
        "      <th>C82</th>\n",
        "      <th>C83</th>\n",
        "      <th>C84</th>\n",
        "      <th>C85</th>\n",
        "      <th>C86</th>\n",
        "      <th>C87</th>\n",
        "      <th>C88</th>\n",
        "      <th>C89</th>\n",
        "      <th>C90</th>\n",
        "      <th>C91</th>\n",
        "      <th>C92</th>\n",
        "      <th>C93</th>\n",
        "      <th>C94</th>\n",
        "      <th>C95</th>\n",
        "      <th>C96</th>\n",
        "      <th>C97</th>\n",
        "      <th>C98</th>\n",
        "      <th>C99</th>\n",
        "      <th>C100</th>\n",
        "      <th>C101</th>\n",
        "      <th>C102</th>\n",
        "      <th>C103</th>\n",
        "      <th>C104</th>\n",
        "      <th>C105</th>\n",
        "      <th>C106</th>\n",
        "      <th>C107</th>\n",
        "      <th>C108</th>\n",
        "      <th>C109</th>\n",
        "      <th>C110</th>\n",
        "      <th>C111</th>\n",
        "      <th>C112</th>\n",
        "      <th>C113</th>\n",
        "      <th>C114</th>\n",
        "      <th>C115</th>\n",
        "      <th>C116</th>\n",
        "      <th>C117</th>\n",
        "      <th>C118</th>\n",
        "      <th>C119</th>\n",
        "      <th>C120</th>\n",
        "      <th>C121</th>\n",
        "      <th>C122</th>\n",
        "      <th>C123</th>\n",
        "      <th>C124</th>\n",
        "      <th>C125</th>\n",
        "      <th>C126</th>\n",
        "      <th>C127</th>\n",
        "      <th>C128</th>\n",
        "      <th>C129</th>\n",
        "      <th>C130</th>\n",
        "      <th>C131</th>\n",
        "      <th>C132</th>\n",
        "      <th>C133</th>\n",
        "      <th>C134</th>\n",
        "      <th>C135</th>\n",
        "      <th>C136</th>\n",
        "      <th>C137</th>\n",
        "      <th>C138</th>\n",
        "      <th>C139</th>\n",
        "      <th>C140</th>\n",
        "      <th>C141</th>\n",
        "      <th>C142</th>\n",
        "      <th>C143</th>\n",
        "      <th>C144</th>\n",
        "      <th>C145</th>\n",
        "      <th>C146</th>\n",
        "      <th>C147</th>\n",
        "      <th>C148</th>\n",
        "      <th>C149</th>\n",
        "      <th>C150</th>\n",
        "      <th>C151</th>\n",
        "      <th>C152</th>\n",
        "      <th>C153</th>\n",
        "      <th>C154</th>\n",
        "      <th>C155</th>\n",
        "      <th>C156</th>\n",
        "      <th>C157</th>\n",
        "      <th>C158</th>\n",
        "      <th>C159</th>\n",
        "      <th>C160</th>\n",
        "      <th>C161</th>\n",
        "      <th>C162</th>\n",
        "      <th>C163</th>\n",
        "      <th>C164</th>\n",
        "      <th>C165</th>\n",
        "      <th>C166</th>\n",
        "      <th>C167</th>\n",
        "      <th>C168</th>\n",
        "      <th>C169</th>\n",
        "      <th>C170</th>\n",
        "      <th>C171</th>\n",
        "      <th>C172</th>\n",
        "      <th>C173</th>\n",
        "      <th>C174</th>\n",
        "      <th>C175</th>\n",
        "      <th>C176</th>\n",
        "      <th>C177</th>\n",
        "      <th>C178</th>\n",
        "      <th>C179</th>\n",
        "      <th>C180</th>\n",
        "      <th>C181</th>\n",
        "      <th>C182</th>\n",
        "      <th>C183</th>\n",
        "      <th>C184</th>\n",
        "      <th>C185</th>\n",
        "      <th>C186</th>\n",
        "      <th>C187</th>\n",
        "      <th>C188</th>\n",
        "      <th>C189</th>\n",
        "      <th>C190</th>\n",
        "      <th>C191</th>\n",
        "      <th>C192</th>\n",
        "      <th>C193</th>\n",
        "      <th>C194</th>\n",
        "      <th>C195</th>\n",
        "      <th>C196</th>\n",
        "      <th>C197</th>\n",
        "      <th>C198</th>\n",
        "      <th>C199</th>\n",
        "      <th>C200</th>\n",
        "      <th>C201</th>\n",
        "      <th>C202</th>\n",
        "      <th>C203</th>\n",
        "      <th>C204</th>\n",
        "      <th>C205</th>\n",
        "      <th>C206</th>\n",
        "      <th>C207</th>\n",
        "      <th>C208</th>\n",
        "      <th>C209</th>\n",
        "      <th>C210</th>\n",
        "      <th>C211</th>\n",
        "      <th>C212</th>\n",
        "      <th>C213</th>\n",
        "      <th>C214</th>\n",
        "      <th>C215</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>93641</th>\n",
        "      <td>N</td>\n",
        "      <td>NVIP</td>\n",
        "      <td>M</td>\n",
        "      <td>56.0</td>\n",
        "      <td>\u5929\u79e4\u5ea7</td>\n",
        "      <td>231</td>\n",
        "      <td>\u64da\u9ede03</td>\n",
        "      <td>N</td>\n",
        "      <td>\u4e2d</td>\n",
        "      <td>4G</td>\n",
        "      <td>1336.0</td>\n",
        "      <td>\u8cfc\u6a5f\u7d04</td>\n",
        "      <td>-63.0</td>\n",
        "      <td>7.0</td>\n",
        "      <td>7.0</td>\n",
        "      <td>\u4f4e\u4e2d</td>\n",
        "      <td>\u4f4e\u4e2d</td>\n",
        "      <td>NaN</td>\n",
        "      <td>4.0</td>\n",
        "      <td>\u7121</td>\n",
        "      <td>N</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>\u7121NP</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>3</td>\n",
        "      <td>2</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>4.556058e+06</td>\n",
        "      <td>314.79</td>\n",
        "      <td>251.08</td>\n",
        "      <td>0.0</td>\n",
        "      <td>32.74</td>\n",
        "      <td>30.97</td>\n",
        "      <td>0.0</td>\n",
        "      <td>9.53</td>\n",
        "      <td>19.23</td>\n",
        "      <td>3.98</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.80</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.10</td>\n",
        "      <td>0.10</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.03</td>\n",
        "      <td>0.06</td>\n",
        "      <td>0.01</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>6.08</td>\n",
        "      <td>59.90</td>\n",
        "      <td>57.95</td>\n",
        "      <td>7.47</td>\n",
        "      <td>41.28</td>\n",
        "      <td>50.87</td>\n",
        "      <td>56.32</td>\n",
        "      <td>34.93</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.02</td>\n",
        "      <td>0.19</td>\n",
        "      <td>0.18</td>\n",
        "      <td>0.02</td>\n",
        "      <td>0.13</td>\n",
        "      <td>0.16</td>\n",
        "      <td>0.18</td>\n",
        "      <td>0.11</td>\n",
        "      <td>0.00</td>\n",
        "      <td>\u65b0\u5317\u5e02</td>\n",
        "      <td>\u677f\u6a4b\u5340</td>\n",
        "      <td>\u65b0\u5317\u5e02</td>\n",
        "      <td>\u677f\u6a4b\u5340</td>\n",
        "      <td>No</td>\n",
        "      <td>No</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>144.0</td>\n",
        "      <td>54.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>175.237</td>\n",
        "      <td>34.535</td>\n",
        "      <td>44.911</td>\n",
        "      <td>0.000000</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>6.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>14.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>72595.0</td>\n",
        "      <td>270462.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>276420.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>291424766.0</td>\n",
        "      <td>186250751.0</td>\n",
        "      <td>257418541.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>13465.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>9375087.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>7764462.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>640707756.0</td>\n",
        "      <td>421879530.0</td>\n",
        "      <td>127221880.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>446369.0</td>\n",
        "      <td>314.80</td>\n",
        "      <td>251.08</td>\n",
        "      <td>32.74</td>\n",
        "      <td>30.97</td>\n",
        "      <td>30.97</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>251.08</td>\n",
        "      <td>204.0</td>\n",
        "      <td>151.0</td>\n",
        "      <td>27.0</td>\n",
        "      <td>26.0</td>\n",
        "      <td>26.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>55.0</td>\n",
        "      <td>30.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>14.0</td>\n",
        "      <td>11.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>7.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>55.0</td>\n",
        "      <td>30.0</td>\n",
        "      <td>14.0</td>\n",
        "      <td>11.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>282.05</td>\n",
        "      <td>251.08</td>\n",
        "      <td>0.0</td>\n",
        "      <td>30.97</td>\n",
        "      <td>0.89</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.11</td>\n",
        "      <td>204.0</td>\n",
        "      <td>151.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>27.0</td>\n",
        "      <td>26.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>10.0</td>\n",
        "      <td>7.0</td>\n",
        "      <td>10.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>177.0</td>\n",
        "      <td>151.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>26.0</td>\n",
        "      <td>\u696d\u8005</td>\n",
        "      <td>31.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>205.0</td>\n",
        "      <td>178910.381836</td>\n",
        "      <td>0.039269</td>\n",
        "      <td>205.0</td>\n",
        "      <td>4.665403e+09</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>48047</th>\n",
        "      <td>Y</td>\n",
        "      <td>NVIP</td>\n",
        "      <td>F</td>\n",
        "      <td>50.0</td>\n",
        "      <td>\u5929\u880d\u5ea7</td>\n",
        "      <td>59</td>\n",
        "      <td>\u64da\u9ede04</td>\n",
        "      <td>Y</td>\n",
        "      <td>\u4e2d\u4f4e</td>\n",
        "      <td>4G</td>\n",
        "      <td>936.0</td>\n",
        "      <td>\u8cfc\u6a5f\u7d04</td>\n",
        "      <td>-37.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>\u4f4e</td>\n",
        "      <td>\u4f4e\u4e2d</td>\n",
        "      <td>3.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>\u7af6\u696dB</td>\n",
        "      <td>N</td>\n",
        "      <td>3</td>\n",
        "      <td>1.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>NaN</td>\n",
        "      <td>2</td>\n",
        "      <td>1.0</td>\n",
        "      <td>NaN</td>\n",
        "      <td>\u7af6\u696dB</td>\n",
        "      <td>58.0</td>\n",
        "      <td>106.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>1.353689e+06</td>\n",
        "      <td>62.46</td>\n",
        "      <td>18.38</td>\n",
        "      <td>0.0</td>\n",
        "      <td>40.28</td>\n",
        "      <td>3.80</td>\n",
        "      <td>0.0</td>\n",
        "      <td>11.30</td>\n",
        "      <td>20.95</td>\n",
        "      <td>2.68</td>\n",
        "      <td>5.35</td>\n",
        "      <td>0.29</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.64</td>\n",
        "      <td>0.06</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.18</td>\n",
        "      <td>0.34</td>\n",
        "      <td>0.04</td>\n",
        "      <td>0.09</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>5.35</td>\n",
        "      <td>9.58</td>\n",
        "      <td>5.78</td>\n",
        "      <td>8.90</td>\n",
        "      <td>5.83</td>\n",
        "      <td>11.75</td>\n",
        "      <td>9.82</td>\n",
        "      <td>2.95</td>\n",
        "      <td>2.50</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.09</td>\n",
        "      <td>0.15</td>\n",
        "      <td>0.09</td>\n",
        "      <td>0.14</td>\n",
        "      <td>0.09</td>\n",
        "      <td>0.19</td>\n",
        "      <td>0.16</td>\n",
        "      <td>0.05</td>\n",
        "      <td>0.04</td>\n",
        "      <td>\u5f70\u5316\u7e23</td>\n",
        "      <td>\u82b1\u58c7\u9109</td>\n",
        "      <td>\u5f70\u5316\u7e23</td>\n",
        "      <td>\u82b1\u58c7\u9109</td>\n",
        "      <td>No</td>\n",
        "      <td>No</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>136.0</td>\n",
        "      <td>67.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>79.081</td>\n",
        "      <td>33.741</td>\n",
        "      <td>18.496</td>\n",
        "      <td>0.989149</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>9.0</td>\n",
        "      <td>7.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>10.0</td>\n",
        "      <td>11.0</td>\n",
        "      <td>17.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>40434.0</td>\n",
        "      <td>240672.0</td>\n",
        "      <td>836016.0</td>\n",
        "      <td>622949.0</td>\n",
        "      <td>6189.0</td>\n",
        "      <td>68801520.0</td>\n",
        "      <td>6478265.0</td>\n",
        "      <td>353809.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>544570.0</td>\n",
        "      <td>16182.0</td>\n",
        "      <td>99075.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>10460676.0</td>\n",
        "      <td>24570.0</td>\n",
        "      <td>3882687.0</td>\n",
        "      <td>9017726.0</td>\n",
        "      <td>385813122.0</td>\n",
        "      <td>171471284.0</td>\n",
        "      <td>10425926.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2793673.0</td>\n",
        "      <td>1184111.0</td>\n",
        "      <td>1177556.0</td>\n",
        "      <td>62.47</td>\n",
        "      <td>18.38</td>\n",
        "      <td>40.28</td>\n",
        "      <td>3.80</td>\n",
        "      <td>3.80</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>20.95</td>\n",
        "      <td>115.0</td>\n",
        "      <td>31.0</td>\n",
        "      <td>68.0</td>\n",
        "      <td>16.0</td>\n",
        "      <td>16.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>25.0</td>\n",
        "      <td>7.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>10.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>25.0</td>\n",
        "      <td>7.0</td>\n",
        "      <td>10.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>22.19</td>\n",
        "      <td>18.38</td>\n",
        "      <td>0.0</td>\n",
        "      <td>3.81</td>\n",
        "      <td>0.83</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.17</td>\n",
        "      <td>115.0</td>\n",
        "      <td>31.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>68.0</td>\n",
        "      <td>16.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>11.0</td>\n",
        "      <td>45.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>7.0</td>\n",
        "      <td>47.0</td>\n",
        "      <td>31.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>16.0</td>\n",
        "      <td>\u7af6\u696dB</td>\n",
        "      <td>32.0</td>\n",
        "      <td>13.0</td>\n",
        "      <td>369.0</td>\n",
        "      <td>0.000000</td>\n",
        "      <td>0.000000</td>\n",
        "      <td>369.0</td>\n",
        "      <td>1.386178e+09</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>104122</th>\n",
        "      <td>Y</td>\n",
        "      <td>NVIP</td>\n",
        "      <td>M</td>\n",
        "      <td>39.0</td>\n",
        "      <td>\u96d9\u5b50\u5ea7</td>\n",
        "      <td>198</td>\n",
        "      <td>\u64da\u9ede06</td>\n",
        "      <td>N</td>\n",
        "      <td>\u4e2d</td>\n",
        "      <td>3G</td>\n",
        "      <td>383.0</td>\n",
        "      <td>\u8cfc\u6a5f\u7d04</td>\n",
        "      <td>-55.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>\u4f4e\u4e2d</td>\n",
        "      <td>\u4f4e\u4e2d</td>\n",
        "      <td>NaN</td>\n",
        "      <td>3.0</td>\n",
        "      <td>\u7af6\u696dA</td>\n",
        "      <td>N</td>\n",
        "      <td>1</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>1.0</td>\n",
        "      <td>1</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>\u7121NP</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>2</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>NaN</td>\n",
        "      <td>109.60</td>\n",
        "      <td>22.57</td>\n",
        "      <td>0.0</td>\n",
        "      <td>75.18</td>\n",
        "      <td>11.85</td>\n",
        "      <td>0.0</td>\n",
        "      <td>65.45</td>\n",
        "      <td>9.73</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.21</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.69</td>\n",
        "      <td>0.11</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.60</td>\n",
        "      <td>0.09</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.23</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.22</td>\n",
        "      <td>3.25</td>\n",
        "      <td>33.70</td>\n",
        "      <td>8.88</td>\n",
        "      <td>10.05</td>\n",
        "      <td>8.55</td>\n",
        "      <td>19.12</td>\n",
        "      <td>18.12</td>\n",
        "      <td>7.48</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.03</td>\n",
        "      <td>0.31</td>\n",
        "      <td>0.08</td>\n",
        "      <td>0.09</td>\n",
        "      <td>0.08</td>\n",
        "      <td>0.17</td>\n",
        "      <td>0.17</td>\n",
        "      <td>0.07</td>\n",
        "      <td>\u53f0\u4e2d\u5e02</td>\n",
        "      <td>\u8c50\u539f\u5340</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>No</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>153.0</td>\n",
        "      <td>25.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>35.122</td>\n",
        "      <td>40.069</td>\n",
        "      <td>10.948</td>\n",
        "      <td>0.000000</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>10.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>21.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>109.60</td>\n",
        "      <td>22.56</td>\n",
        "      <td>75.18</td>\n",
        "      <td>11.85</td>\n",
        "      <td>11.85</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>65.45</td>\n",
        "      <td>81.0</td>\n",
        "      <td>24.0</td>\n",
        "      <td>50.0</td>\n",
        "      <td>7.0</td>\n",
        "      <td>7.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>16.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>6.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>16.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>34.42</td>\n",
        "      <td>22.56</td>\n",
        "      <td>0.0</td>\n",
        "      <td>11.86</td>\n",
        "      <td>0.66</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.34</td>\n",
        "      <td>81.0</td>\n",
        "      <td>24.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>50.0</td>\n",
        "      <td>7.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>43.0</td>\n",
        "      <td>7.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>31.0</td>\n",
        "      <td>24.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>7.0</td>\n",
        "      <td>\u7af6\u696dA</td>\n",
        "      <td>17.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4425</th>\n",
        "      <td>N</td>\n",
        "      <td>NVIP</td>\n",
        "      <td>M</td>\n",
        "      <td>31.0</td>\n",
        "      <td>\u5de8\u87f9\u5ea7</td>\n",
        "      <td>1</td>\n",
        "      <td>\u64da\u9ede12</td>\n",
        "      <td>Y</td>\n",
        "      <td>\u9ad8</td>\n",
        "      <td>4G</td>\n",
        "      <td>636.0</td>\n",
        "      <td>\u8cfc\u6a5f\u7d04</td>\n",
        "      <td>26.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>\u96f6\u5143</td>\n",
        "      <td>\u96f6\u5143</td>\n",
        "      <td>NaN</td>\n",
        "      <td>1.0</td>\n",
        "      <td>\u7121</td>\n",
        "      <td>N</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>\u7121NP</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>4</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>3</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>7.276558e+05</td>\n",
        "      <td>501.69</td>\n",
        "      <td>28.72</td>\n",
        "      <td>0.0</td>\n",
        "      <td>8.80</td>\n",
        "      <td>10.10</td>\n",
        "      <td>0.0</td>\n",
        "      <td>4.10</td>\n",
        "      <td>4.70</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.06</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.02</td>\n",
        "      <td>0.02</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.91</td>\n",
        "      <td>0.01</td>\n",
        "      <td>0.01</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>85.45</td>\n",
        "      <td>40.93</td>\n",
        "      <td>0.0</td>\n",
        "      <td>58.48</td>\n",
        "      <td>8.38</td>\n",
        "      <td>74.70</td>\n",
        "      <td>9.32</td>\n",
        "      <td>4.62</td>\n",
        "      <td>15.13</td>\n",
        "      <td>11.77</td>\n",
        "      <td>3.42</td>\n",
        "      <td>189.48</td>\n",
        "      <td>0.17</td>\n",
        "      <td>0.08</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.12</td>\n",
        "      <td>0.02</td>\n",
        "      <td>0.15</td>\n",
        "      <td>0.02</td>\n",
        "      <td>0.01</td>\n",
        "      <td>0.03</td>\n",
        "      <td>0.02</td>\n",
        "      <td>0.01</td>\n",
        "      <td>0.38</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>No</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>501.68</td>\n",
        "      <td>28.72</td>\n",
        "      <td>8.80</td>\n",
        "      <td>10.10</td>\n",
        "      <td>10.10</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>199.29</td>\n",
        "      <td>105.0</td>\n",
        "      <td>23.0</td>\n",
        "      <td>9.0</td>\n",
        "      <td>16.0</td>\n",
        "      <td>16.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>31.0</td>\n",
        "      <td>9.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>9.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>31.0</td>\n",
        "      <td>9.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>9.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>38.80</td>\n",
        "      <td>28.71</td>\n",
        "      <td>0.0</td>\n",
        "      <td>10.09</td>\n",
        "      <td>0.74</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.26</td>\n",
        "      <td>105.0</td>\n",
        "      <td>23.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>9.0</td>\n",
        "      <td>16.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>6.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>39.0</td>\n",
        "      <td>23.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>16.0</td>\n",
        "      <td>\u696d\u8005</td>\n",
        "      <td>11.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>247.0</td>\n",
        "      <td>71579.413086</td>\n",
        "      <td>0.098370</td>\n",
        "      <td>247.0</td>\n",
        "      <td>7.451196e+08</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>108463</th>\n",
        "      <td>N</td>\n",
        "      <td>NVIP</td>\n",
        "      <td>M</td>\n",
        "      <td>79.0</td>\n",
        "      <td>\u5929\u79e4\u5ea7</td>\n",
        "      <td>87</td>\n",
        "      <td>\u64da\u9ede06</td>\n",
        "      <td>N</td>\n",
        "      <td>\u7121</td>\n",
        "      <td>3G</td>\n",
        "      <td>589.0</td>\n",
        "      <td>\u8cfc\u6a5f\u7d04</td>\n",
        "      <td>-65.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>\u4f4e\u4e2d</td>\n",
        "      <td>\u4f4e</td>\n",
        "      <td>1.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>\u7121</td>\n",
        "      <td>N</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>\u7121NP</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "       C3    C4 C5    C6   C7   C8    C9 C10 C11 C12     C13  C14   C15  C16  \\\n",
        "93641   N  NVIP  M  56.0  \u5929\u79e4\u5ea7  231  \u64da\u9ede03   N   \u4e2d  4G  1336.0  \u8cfc\u6a5f\u7d04 -63.0  7.0   \n",
        "48047   Y  NVIP  F  50.0  \u5929\u880d\u5ea7   59  \u64da\u9ede04   Y  \u4e2d\u4f4e  4G   936.0  \u8cfc\u6a5f\u7d04 -37.0  2.0   \n",
        "104122  Y  NVIP  M  39.0  \u96d9\u5b50\u5ea7  198  \u64da\u9ede06   N   \u4e2d  3G   383.0  \u8cfc\u6a5f\u7d04 -55.0  5.0   \n",
        "4425    N  NVIP  M  31.0  \u5de8\u87f9\u5ea7    1  \u64da\u9ede12   Y   \u9ad8  4G   636.0  \u8cfc\u6a5f\u7d04  26.0  1.0   \n",
        "108463  N  NVIP  M  79.0  \u5929\u79e4\u5ea7   87  \u64da\u9ede06   N   \u7121  3G   589.0  \u8cfc\u6a5f\u7d04 -65.0  4.0   \n",
        "\n",
        "        C17 C18 C19  C20  C21  C22 C23  C24  C25  C26  C27  C28  C29  C30  \\\n",
        "93641   7.0  \u4f4e\u4e2d  \u4f4e\u4e2d  NaN  4.0    \u7121   N    0  0.0  0.0  0.0    0  0.0  0.0   \n",
        "48047   2.0   \u4f4e  \u4f4e\u4e2d  3.0  2.0  \u7af6\u696dB   N    3  1.0  2.0  NaN    2  1.0  NaN   \n",
        "104122  5.0  \u4f4e\u4e2d  \u4f4e\u4e2d  NaN  3.0  \u7af6\u696dA   N    1  NaN  NaN  1.0    1  NaN  NaN   \n",
        "4425    1.0  \u96f6\u5143  \u96f6\u5143  NaN  1.0    \u7121   N    0  0.0  0.0  0.0    0  0.0  0.0   \n",
        "108463  4.0  \u4f4e\u4e2d   \u4f4e  1.0  4.0    \u7121   N    0  0.0  0.0  0.0    0  0.0  0.0   \n",
        "\n",
        "        C31   C32    C33  C34  C35  C36  C37  C38  C39  C40  C41  C42  C43  \\\n",
        "93641   \u7121NP   NaN    NaN  NaN    3    2    0    1    0    0    0    0    1   \n",
        "48047   \u7af6\u696dB  58.0  106.0  2.0    1    1    0    0    0    0    0    0    1   \n",
        "104122  \u7121NP   NaN    NaN  NaN    2    1    0    1    0    0    0    0    1   \n",
        "4425    \u7121NP   NaN    NaN  NaN    4    1    1    0    0    1    1    0    3   \n",
        "108463  \u7121NP   NaN    NaN  NaN    1    1    0    0    0    0    0    0    1   \n",
        "\n",
        "        C44  C45 C46           C47     C48     C49  C50    C51    C52  C53  \\\n",
        "93641   NaN  NaN  No  4.556058e+06  314.79  251.08  0.0  32.74  30.97  0.0   \n",
        "48047   NaN  NaN  No  1.353689e+06   62.46   18.38  0.0  40.28   3.80  0.0   \n",
        "104122  NaN  NaN  No           NaN  109.60   22.57  0.0  75.18  11.85  0.0   \n",
        "4425    NaN  NaN  No  7.276558e+05  501.69   28.72  0.0   8.80  10.10  0.0   \n",
        "108463  NaN  NaN  No           NaN     NaN     NaN  NaN    NaN    NaN  NaN   \n",
        "\n",
        "          C54    C55   C56   C57   C58  C59   C60   C61  C62   C63   C64  \\\n",
        "93641    9.53  19.23  3.98  0.00  0.80  0.0  0.10  0.10  0.0  0.00  0.03   \n",
        "48047   11.30  20.95  2.68  5.35  0.29  0.0  0.64  0.06  0.0  0.00  0.18   \n",
        "104122  65.45   9.73  0.00  0.00  0.21  0.0  0.69  0.11  0.0  0.00  0.60   \n",
        "4425     4.10   4.70  0.00  0.00  0.06  0.0  0.02  0.02  0.0  0.91  0.01   \n",
        "108463    NaN    NaN   NaN   NaN   NaN  NaN   NaN   NaN  NaN   NaN   NaN   \n",
        "\n",
        "         C65   C66   C67    C68    C69  C70    C71    C72    C73   C74    C75  \\\n",
        "93641   0.06  0.01  0.00   0.00   0.00  0.0   6.08  59.90  57.95  7.47  41.28   \n",
        "48047   0.34  0.04  0.09   0.00   0.00  0.0   5.35   9.58   5.78  8.90   5.83   \n",
        "104122  0.09  0.00  0.00   0.23   0.00  0.0   0.22   3.25  33.70  8.88  10.05   \n",
        "4425    0.01  0.00  0.00  85.45  40.93  0.0  58.48   8.38  74.70  9.32   4.62   \n",
        "108463   NaN   NaN   NaN    NaN    NaN  NaN    NaN    NaN    NaN   NaN    NaN   \n",
        "\n",
        "          C76    C77    C78     C79   C80   C81  C82   C83   C84   C85   C86  \\\n",
        "93641   50.87  56.32  34.93    0.00  0.00  0.00  0.0  0.02  0.19  0.18  0.02   \n",
        "48047   11.75   9.82   2.95    2.50  0.00  0.00  0.0  0.09  0.15  0.09  0.14   \n",
        "104122   8.55  19.12  18.12    7.48  0.00  0.00  0.0  0.00  0.03  0.31  0.08   \n",
        "4425    15.13  11.77   3.42  189.48  0.17  0.08  0.0  0.12  0.02  0.15  0.02   \n",
        "108463    NaN    NaN    NaN     NaN   NaN   NaN  NaN   NaN   NaN   NaN   NaN   \n",
        "\n",
        "         C87   C88   C89   C90   C91  C92  C93  C94  C95  C96  C97  C98  C99  \\\n",
        "93641   0.13  0.16  0.18  0.11  0.00  \u65b0\u5317\u5e02  \u677f\u6a4b\u5340  \u65b0\u5317\u5e02  \u677f\u6a4b\u5340   No   No  0.0  0.0   \n",
        "48047   0.09  0.19  0.16  0.05  0.04  \u5f70\u5316\u7e23  \u82b1\u58c7\u9109  \u5f70\u5316\u7e23  \u82b1\u58c7\u9109   No   No  0.0  0.0   \n",
        "104122  0.09  0.08  0.17  0.17  0.07  \u53f0\u4e2d\u5e02  \u8c50\u539f\u5340  NaN  NaN   No   No  0.0  0.0   \n",
        "4425    0.01  0.03  0.02  0.01  0.38  NaN  NaN  NaN  NaN   No   No  NaN  NaN   \n",
        "108463   NaN   NaN   NaN   NaN   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
        "\n",
        "         C100  C101  C102     C103    C104    C105      C106  C107  C108  \\\n",
        "93641   144.0  54.0   0.0  175.237  34.535  44.911  0.000000   1.0   0.0   \n",
        "48047   136.0  67.0   0.0   79.081  33.741  18.496  0.989149   0.0   0.0   \n",
        "104122  153.0  25.0   0.0   35.122  40.069  10.948  0.000000   0.0   0.0   \n",
        "4425      NaN   NaN   NaN      NaN     NaN     NaN       NaN   NaN   NaN   \n",
        "108463    NaN   NaN   NaN      NaN     NaN     NaN       NaN   NaN   NaN   \n",
        "\n",
        "        C109  C110  C111  C112  C113  C114  C115  C116  C117  C118  C119  \\\n",
        "93641    0.0   0.0   8.0   2.0   4.0   1.0   8.0   5.0   0.0   6.0   4.0   \n",
        "48047    0.0   0.0   0.0   0.0   1.0   1.0   9.0   7.0   0.0  10.0  11.0   \n",
        "104122   0.0   0.0   8.0   0.0   8.0   1.0  10.0   0.0   0.0   2.0   3.0   \n",
        "4425     NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
        "108463   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
        "\n",
        "        C120  C121  C122     C123      C124      C125      C126    C127  \\\n",
        "93641   14.0   5.0   0.0  72595.0  270462.0       0.0  276420.0     0.0   \n",
        "48047   17.0   2.0   2.0  40434.0  240672.0  836016.0  622949.0  6189.0   \n",
        "104122  21.0   1.0   0.0      0.0       0.0       0.0       0.0     0.0   \n",
        "4425     NaN   NaN   NaN      NaN       NaN       NaN       NaN     NaN   \n",
        "108463   NaN   NaN   NaN      NaN       NaN       NaN       NaN     NaN   \n",
        "\n",
        "               C128         C129         C130  C131      C132     C133  \\\n",
        "93641   291424766.0  186250751.0  257418541.0   0.0       0.0      0.0   \n",
        "48047    68801520.0    6478265.0     353809.0   0.0  544570.0  16182.0   \n",
        "104122          0.0          0.0          0.0   0.0       0.0      0.0   \n",
        "4425            NaN          NaN          NaN   NaN       NaN      NaN   \n",
        "108463          NaN          NaN          NaN   NaN       NaN      NaN   \n",
        "\n",
        "           C134  C135        C136     C137       C138       C139         C140  \\\n",
        "93641   13465.0   0.0   9375087.0      0.0  7764462.0        0.0  640707756.0   \n",
        "48047   99075.0   0.0  10460676.0  24570.0  3882687.0  9017726.0  385813122.0   \n",
        "104122      0.0   0.0         0.0      0.0        0.0        0.0          0.0   \n",
        "4425        NaN   NaN         NaN      NaN        NaN        NaN          NaN   \n",
        "108463      NaN   NaN         NaN      NaN        NaN        NaN          NaN   \n",
        "\n",
        "               C141         C142  C143       C144       C145       C146  \\\n",
        "93641   421879530.0  127221880.0   0.0        0.0        0.0   446369.0   \n",
        "48047   171471284.0   10425926.0   0.0  2793673.0  1184111.0  1177556.0   \n",
        "104122          0.0          0.0   0.0        0.0        0.0        0.0   \n",
        "4425            NaN          NaN   NaN        NaN        NaN        NaN   \n",
        "108463          NaN          NaN   NaN        NaN        NaN        NaN   \n",
        "\n",
        "          C147    C148   C149   C150   C151  C152  C153    C154   C155   C156  \\\n",
        "93641   314.80  251.08  32.74  30.97  30.97   0.0   0.0  251.08  204.0  151.0   \n",
        "48047    62.47   18.38  40.28   3.80   3.80   0.0   0.0   20.95  115.0   31.0   \n",
        "104122  109.60   22.56  75.18  11.85  11.85   0.0   0.0   65.45   81.0   24.0   \n",
        "4425    501.68   28.72   8.80  10.10  10.10   0.0   0.0  199.29  105.0   23.0   \n",
        "108463     NaN     NaN    NaN    NaN    NaN   NaN   NaN     NaN    NaN    NaN   \n",
        "\n",
        "        C157  C158  C159  C160  C161  C162  C163  C164  C165  C166  C167  \\\n",
        "93641   27.0  26.0  26.0   0.0   0.0  55.0  30.0   0.0  14.0  11.0   0.0   \n",
        "48047   68.0  16.0  16.0   0.0   0.0  25.0   7.0   0.0  10.0   8.0   0.0   \n",
        "104122  50.0   7.0   7.0   0.0   0.0  16.0   4.0   0.0   8.0   4.0   0.0   \n",
        "4425     9.0  16.0  16.0   0.0   0.0  31.0   9.0   0.0   5.0   9.0   0.0   \n",
        "108463   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
        "\n",
        "        C168  C169  C170  C171  C172  C173  C174  C175  C176  C177  C178  \\\n",
        "93641    7.0   5.0   2.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
        "48047    5.0   3.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
        "104122   6.0   2.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
        "4425     4.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
        "108463   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
        "\n",
        "        C179  C180  C181  C182  C183  C184  C185  C186    C187    C188  C189  \\\n",
        "93641    0.0   0.0   0.0  55.0  30.0  14.0  11.0   0.0  282.05  251.08   0.0   \n",
        "48047    0.0   0.0   0.0  25.0   7.0  10.0   8.0   0.0   22.19   18.38   0.0   \n",
        "104122   0.0   0.0   0.0  16.0   4.0   8.0   4.0   0.0   34.42   22.56   0.0   \n",
        "4425     0.0   0.0   0.0  31.0   9.0   5.0   9.0   0.0   38.80   28.71   0.0   \n",
        "108463   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN    0.00    0.00   0.0   \n",
        "\n",
        "         C190  C191  C192  C193   C194   C195  C196  C197  C198  C199  C200  \\\n",
        "93641   30.97  0.89   0.0  0.11  204.0  151.0   0.0  27.0  26.0   0.0  10.0   \n",
        "48047    3.81  0.83   0.0  0.17  115.0   31.0   0.0  68.0  16.0   0.0  11.0   \n",
        "104122  11.86  0.66   0.0  0.34   81.0   24.0   0.0  50.0   7.0   0.0  43.0   \n",
        "4425    10.09  0.74   0.0  0.26  105.0   23.0   0.0   9.0  16.0   0.0   6.0   \n",
        "108463   0.00  0.00   0.0  0.00    NaN    NaN   NaN   NaN   NaN   NaN   NaN   \n",
        "\n",
        "        C201  C202  C203   C204   C205  C206  C207 C208  C209  C210   C211  \\\n",
        "93641    7.0  10.0   0.0  177.0  151.0   0.0  26.0   \u696d\u8005  31.0   1.0  205.0   \n",
        "48047   45.0   5.0   7.0   47.0   31.0   0.0  16.0  \u7af6\u696dB  32.0  13.0  369.0   \n",
        "104122   7.0   0.0   0.0   31.0   24.0   0.0   7.0  \u7af6\u696dA  17.0   0.0    0.0   \n",
        "4425     3.0   0.0   0.0   39.0   23.0   0.0  16.0   \u696d\u8005  11.0   3.0  247.0   \n",
        "108463   NaN   NaN   NaN    0.0    0.0   0.0   0.0  NaN   NaN   0.0    0.0   \n",
        "\n",
        "                 C212      C213   C214          C215  \n",
        "93641   178910.381836  0.039269  205.0  4.665403e+09  \n",
        "48047        0.000000  0.000000  369.0  1.386178e+09  \n",
        "104122            NaN       NaN    0.0  0.000000e+00  \n",
        "4425     71579.413086  0.098370  247.0  7.451196e+08  \n",
        "108463            NaN       NaN    NaN           NaN  "
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
      "_df = pd.read_csv('../../MB_PCUST_NPOUT_EXAM.txt', encoding='utf-8', sep='\\t', names=['C2']+['C'+str(i+4) for i in xrange(212)], na_values=['?', '\\N'])\n",
      "_df = pd.concat([df, _df], join='inner')\n",
      "\n",
      "encoder = {col: {cat:n for n, cat in enumerate(_df[col].unique())} for col in _df.select_dtypes(include=['object']).columns}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Column Independency"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dfco = df.corr()\n",
      "indices = np.where(dfco > 0.8)\n",
      "cols = [(dfco.index[x], dfco.columns[y]) for x, y in zip(*indices) if x != y and x < y]\n",
      "\n",
      "for col in cols:\n",
      "  if col[1] in df.columns and col[1]!='C3': del df[col[1]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##chi-squared test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import chi2_contingency\n",
      "\n",
      "for col in df.select_dtypes(include=['object']).columns:\n",
      "    if col=='C3': continue\n",
      "    chi2, p, dof, expected = chi2_contingency(pd.crosstab(df[col], df['C3']))\n",
      "    if p>=0.05: # H0 (independent - no affects)\"\n",
      "        print \"%s does NOT affect C3\" % col\n",
      "        del df[col]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C5 does NOT affect C3\n",
        "C44 does NOT affect C3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C45 does NOT affect C3\n",
        "C46 does NOT affect C3\n",
        "C95 does NOT affect C3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C96 does NOT affect C3\n",
        "C97 does NOT affect C3\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##anova test"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import f_oneway\n",
      "\n",
      "for col in df.select_dtypes(exclude=['object']).columns:\n",
      "    df_y = df.loc[df['C3']=='Y', col].fillna(0) # or dropna ?\n",
      "    df_n = df.loc[df['C3']=='N', col].fillna(0) # or dropna ?\n",
      "    fVal, p = f_oneway(df_y, df_n)\n",
      "    if p>=0.05: # H0 (%s df_y's mean = df_n's mean)\n",
      "        print \"%s no difference\" % (col)\n",
      "        del df[col]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C47 no difference\n",
        "C50 no difference\n",
        "C52 no difference\n",
        "C53 no difference\n",
        "C61 no difference"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C62 no difference\n",
        "C68 no difference\n",
        "C69 no difference\n",
        "C70 no difference"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C71 no difference\n",
        "C74 no difference\n",
        "C77 no difference\n",
        "C78 no difference\n",
        "C79 no difference"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C81 no difference\n",
        "C82 no difference\n",
        "C83 no difference\n",
        "C85 no difference\n",
        "C86 no difference\n",
        "C87 no difference"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C108 no difference\n",
        "C109 no difference\n",
        "C112 no difference"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C115 no difference\n",
        "C117 no difference\n",
        "C118 no difference\n",
        "C119 no difference\n",
        "C121 no difference"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C122 no difference\n",
        "C125 no difference\n",
        "C126 no difference\n",
        "C127 no difference\n",
        "C128 no difference\n",
        "C130 no difference"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C131 no difference\n",
        "C132 no difference\n",
        "C133 no difference\n",
        "C134 no difference\n",
        "C135 no difference\n",
        "C136 no difference\n",
        "C137 no difference"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C140 no difference\n",
        "C143 no difference\n",
        "C144 no difference\n",
        "C145 no difference\n",
        "C152 no difference"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C155 no difference\n",
        "C158 no difference\n",
        "C160 no difference\n",
        "C166 no difference"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C173 no difference\n",
        "C175 no difference\n",
        "C177 no difference"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C178 no difference\n",
        "C179 no difference\n",
        "C180 no difference\n",
        "C181 no difference\n",
        "C189 no difference\n",
        "C191 no difference\n",
        "C209 no difference"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "C210 no difference\n",
        "C212 no difference\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(df.columns), \", \".join(df.columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "92 C3, C4, C6, C7, C8, C9, C10, C11, C12, C13, C14, C15, C16, C18, C19, C20, C22, C23, C24, C31, C33, C34, C35, C37, C38, C39, C40, C42, C43, C48, C51, C55, C56, C57, C58, C59, C60, C63, C64, C65, C66, C67, C72, C73, C75, C76, C80, C84, C88, C89, C90, C91, C92, C93, C94, C98, C99, C100, C102, C103, C104, C106, C107, C110, C111, C113, C114, C116, C120, C123, C124, C129, C138, C139, C141, C146, C157, C161, C164, C170, C171, C172, C174, C176, C192, C196, C201, C202, C203, C208, C211, C213\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Preprocessing (for Feature Selection)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "dfc = df.loc[:, df.select_dtypes(exclude=['object']).columns]\n",
      "for col in dfc.columns:\n",
      "    dfc[col].fillna(0, inplace=True)\n",
      "    dfc[col] = StandardScaler().fit_transform(dfc[col]) #(df['DT1_DATA_RTD_DUR_L'] - df['DT1_DATA_RTD_DUR_L'].mean())/df['DT1_DATA_RTD_DUR_L'].std(ddof=0)\n",
      "\n",
      "dfc.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>C6</th>\n",
        "      <th>C8</th>\n",
        "      <th>C13</th>\n",
        "      <th>C15</th>\n",
        "      <th>C16</th>\n",
        "      <th>C20</th>\n",
        "      <th>C24</th>\n",
        "      <th>C33</th>\n",
        "      <th>C34</th>\n",
        "      <th>C35</th>\n",
        "      <th>C37</th>\n",
        "      <th>C38</th>\n",
        "      <th>C39</th>\n",
        "      <th>C40</th>\n",
        "      <th>C42</th>\n",
        "      <th>C43</th>\n",
        "      <th>C48</th>\n",
        "      <th>C51</th>\n",
        "      <th>C55</th>\n",
        "      <th>C56</th>\n",
        "      <th>C57</th>\n",
        "      <th>C58</th>\n",
        "      <th>C59</th>\n",
        "      <th>C60</th>\n",
        "      <th>C63</th>\n",
        "      <th>C64</th>\n",
        "      <th>C65</th>\n",
        "      <th>C66</th>\n",
        "      <th>C67</th>\n",
        "      <th>C72</th>\n",
        "      <th>C73</th>\n",
        "      <th>C75</th>\n",
        "      <th>C76</th>\n",
        "      <th>C80</th>\n",
        "      <th>C84</th>\n",
        "      <th>C88</th>\n",
        "      <th>C89</th>\n",
        "      <th>C90</th>\n",
        "      <th>C91</th>\n",
        "      <th>C98</th>\n",
        "      <th>C99</th>\n",
        "      <th>C100</th>\n",
        "      <th>C102</th>\n",
        "      <th>C103</th>\n",
        "      <th>C104</th>\n",
        "      <th>C106</th>\n",
        "      <th>C107</th>\n",
        "      <th>C110</th>\n",
        "      <th>C111</th>\n",
        "      <th>C113</th>\n",
        "      <th>C114</th>\n",
        "      <th>C116</th>\n",
        "      <th>C120</th>\n",
        "      <th>C123</th>\n",
        "      <th>C124</th>\n",
        "      <th>C129</th>\n",
        "      <th>C138</th>\n",
        "      <th>C139</th>\n",
        "      <th>C141</th>\n",
        "      <th>C146</th>\n",
        "      <th>C157</th>\n",
        "      <th>C161</th>\n",
        "      <th>C164</th>\n",
        "      <th>C170</th>\n",
        "      <th>C171</th>\n",
        "      <th>C172</th>\n",
        "      <th>C174</th>\n",
        "      <th>C176</th>\n",
        "      <th>C192</th>\n",
        "      <th>C196</th>\n",
        "      <th>C201</th>\n",
        "      <th>C202</th>\n",
        "      <th>C203</th>\n",
        "      <th>C211</th>\n",
        "      <th>C213</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>93641</th>\n",
        "      <td>0.821162</td>\n",
        "      <td>1.774782</td>\n",
        "      <td>1.881226</td>\n",
        "      <td>-1.162200</td>\n",
        "      <td>2.498323</td>\n",
        "      <td>-1.094457</td>\n",
        "      <td>-0.779034</td>\n",
        "      <td>-0.578577</td>\n",
        "      <td>-0.603663</td>\n",
        "      <td>-0.033586</td>\n",
        "      <td>-0.180543</td>\n",
        "      <td>0.322881</td>\n",
        "      <td>-0.044217</td>\n",
        "      <td>-0.508426</td>\n",
        "      <td>-0.09914</td>\n",
        "      <td>-0.433263</td>\n",
        "      <td>1.629813</td>\n",
        "      <td>0.402607</td>\n",
        "      <td>0.606868</td>\n",
        "      <td>0.361610</td>\n",
        "      <td>-0.173105</td>\n",
        "      <td>1.071462</td>\n",
        "      <td>-0.238164</td>\n",
        "      <td>-0.640967</td>\n",
        "      <td>-0.262700</td>\n",
        "      <td>-0.479476</td>\n",
        "      <td>-0.336385</td>\n",
        "      <td>-0.098944</td>\n",
        "      <td>-0.229523</td>\n",
        "      <td>3.271858</td>\n",
        "      <td>2.544756</td>\n",
        "      <td>1.655106</td>\n",
        "      <td>1.822882</td>\n",
        "      <td>-0.219719</td>\n",
        "      <td>0.880828</td>\n",
        "      <td>0.116325</td>\n",
        "      <td>0.273392</td>\n",
        "      <td>0.002798</td>\n",
        "      <td>-0.473218</td>\n",
        "      <td>-0.218817</td>\n",
        "      <td>-0.237575</td>\n",
        "      <td>-0.096930</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.306521</td>\n",
        "      <td>-0.026534</td>\n",
        "      <td>-0.241267</td>\n",
        "      <td>-0.270795</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.083393</td>\n",
        "      <td>0.199553</td>\n",
        "      <td>-0.079576</td>\n",
        "      <td>0.615286</td>\n",
        "      <td>0.902614</td>\n",
        "      <td>-0.151347</td>\n",
        "      <td>-0.186144</td>\n",
        "      <td>0.018421</td>\n",
        "      <td>-0.188252</td>\n",
        "      <td>-0.214552</td>\n",
        "      <td>0.008912</td>\n",
        "      <td>-0.150861</td>\n",
        "      <td>0.523532</td>\n",
        "      <td>-0.071856</td>\n",
        "      <td>-0.301045</td>\n",
        "      <td>1.823102</td>\n",
        "      <td>-0.395999</td>\n",
        "      <td>-0.181818</td>\n",
        "      <td>-0.120873</td>\n",
        "      <td>0.0</td>\n",
        "      <td>-0.033185</td>\n",
        "      <td>-0.184258</td>\n",
        "      <td>0.078469</td>\n",
        "      <td>1.921761</td>\n",
        "      <td>-0.234874</td>\n",
        "      <td>0.078501</td>\n",
        "      <td>-0.043174</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>48047</th>\n",
        "      <td>0.388899</td>\n",
        "      <td>-0.578349</td>\n",
        "      <td>0.905957</td>\n",
        "      <td>-0.278162</td>\n",
        "      <td>-0.148991</td>\n",
        "      <td>0.595215</td>\n",
        "      <td>2.081907</td>\n",
        "      <td>2.635992</td>\n",
        "      <td>1.961841</td>\n",
        "      <td>-0.192987</td>\n",
        "      <td>-0.180543</td>\n",
        "      <td>-0.573762</td>\n",
        "      <td>-0.044217</td>\n",
        "      <td>-0.508426</td>\n",
        "      <td>-0.09914</td>\n",
        "      <td>-0.433263</td>\n",
        "      <td>-0.059265</td>\n",
        "      <td>0.606385</td>\n",
        "      <td>0.697688</td>\n",
        "      <td>0.190255</td>\n",
        "      <td>0.782200</td>\n",
        "      <td>-0.523683</td>\n",
        "      <td>-0.238164</td>\n",
        "      <td>1.366237</td>\n",
        "      <td>-0.262700</td>\n",
        "      <td>0.334649</td>\n",
        "      <td>1.208843</td>\n",
        "      <td>0.315280</td>\n",
        "      <td>1.108718</td>\n",
        "      <td>0.212730</td>\n",
        "      <td>-0.165096</td>\n",
        "      <td>-0.104519</td>\n",
        "      <td>0.085750</td>\n",
        "      <td>-0.219719</td>\n",
        "      <td>0.553273</td>\n",
        "      <td>0.310891</td>\n",
        "      <td>0.144064</td>\n",
        "      <td>-0.407700</td>\n",
        "      <td>-0.146455</td>\n",
        "      <td>-0.218817</td>\n",
        "      <td>-0.237575</td>\n",
        "      <td>-0.152039</td>\n",
        "      <td>0.0</td>\n",
        "      <td>-0.116993</td>\n",
        "      <td>-0.038301</td>\n",
        "      <td>-0.037996</td>\n",
        "      <td>-0.407168</td>\n",
        "      <td>0.0</td>\n",
        "      <td>-0.576629</td>\n",
        "      <td>-0.463414</td>\n",
        "      <td>-0.079576</td>\n",
        "      <td>1.102873</td>\n",
        "      <td>1.308600</td>\n",
        "      <td>-0.151486</td>\n",
        "      <td>-0.186296</td>\n",
        "      <td>-0.325857</td>\n",
        "      <td>-0.193587</td>\n",
        "      <td>-0.198017</td>\n",
        "      <td>-0.214294</td>\n",
        "      <td>-0.147224</td>\n",
        "      <td>2.183700</td>\n",
        "      <td>-0.071856</td>\n",
        "      <td>-0.301045</td>\n",
        "      <td>0.735851</td>\n",
        "      <td>1.137692</td>\n",
        "      <td>-0.181818</td>\n",
        "      <td>-0.120873</td>\n",
        "      <td>0.0</td>\n",
        "      <td>-0.033185</td>\n",
        "      <td>-0.184258</td>\n",
        "      <td>3.237174</td>\n",
        "      <td>0.861830</td>\n",
        "      <td>1.920218</td>\n",
        "      <td>0.535968</td>\n",
        "      <td>-0.570177</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>104122</th>\n",
        "      <td>-0.403582</td>\n",
        "      <td>1.323309</td>\n",
        "      <td>-0.442353</td>\n",
        "      <td>-0.890188</td>\n",
        "      <td>1.439397</td>\n",
        "      <td>-1.094457</td>\n",
        "      <td>0.174613</td>\n",
        "      <td>-0.578577</td>\n",
        "      <td>-0.603663</td>\n",
        "      <td>-0.113287</td>\n",
        "      <td>-0.180543</td>\n",
        "      <td>0.322881</td>\n",
        "      <td>-0.044217</td>\n",
        "      <td>-0.508426</td>\n",
        "      <td>-0.09914</td>\n",
        "      <td>-0.433263</td>\n",
        "      <td>0.256287</td>\n",
        "      <td>1.549600</td>\n",
        "      <td>0.105246</td>\n",
        "      <td>-0.162999</td>\n",
        "      <td>-0.173105</td>\n",
        "      <td>-0.773902</td>\n",
        "      <td>-0.238164</td>\n",
        "      <td>1.552089</td>\n",
        "      <td>-0.262700</td>\n",
        "      <td>2.614200</td>\n",
        "      <td>-0.170825</td>\n",
        "      <td>-0.237019</td>\n",
        "      <td>-0.229523</td>\n",
        "      <td>-0.172093</td>\n",
        "      <td>1.285145</td>\n",
        "      <td>0.104948</td>\n",
        "      <td>-0.056346</td>\n",
        "      <td>-0.219719</td>\n",
        "      <td>-0.429392</td>\n",
        "      <td>-0.402518</td>\n",
        "      <td>0.208728</td>\n",
        "      <td>0.413297</td>\n",
        "      <td>0.098617</td>\n",
        "      <td>-0.218817</td>\n",
        "      <td>-0.237575</td>\n",
        "      <td>-0.034932</td>\n",
        "      <td>0.0</td>\n",
        "      <td>-0.310608</td>\n",
        "      <td>0.055478</td>\n",
        "      <td>-0.241267</td>\n",
        "      <td>-0.407168</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.083393</td>\n",
        "      <td>1.083510</td>\n",
        "      <td>-0.079576</td>\n",
        "      <td>-0.603681</td>\n",
        "      <td>1.849914</td>\n",
        "      <td>-0.151661</td>\n",
        "      <td>-0.187524</td>\n",
        "      <td>-0.338263</td>\n",
        "      <td>-0.198922</td>\n",
        "      <td>-0.214552</td>\n",
        "      <td>-0.367138</td>\n",
        "      <td>-0.153082</td>\n",
        "      <td>1.454846</td>\n",
        "      <td>-0.071856</td>\n",
        "      <td>-0.301045</td>\n",
        "      <td>-0.351399</td>\n",
        "      <td>-0.395999</td>\n",
        "      <td>-0.181818</td>\n",
        "      <td>-0.120873</td>\n",
        "      <td>0.0</td>\n",
        "      <td>-0.033185</td>\n",
        "      <td>-0.184258</td>\n",
        "      <td>0.078469</td>\n",
        "      <td>-0.198101</td>\n",
        "      <td>-0.234874</td>\n",
        "      <td>-0.493332</td>\n",
        "      <td>-0.570177</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4425</th>\n",
        "      <td>-0.979932</td>\n",
        "      <td>-1.371847</td>\n",
        "      <td>0.174504</td>\n",
        "      <td>1.863928</td>\n",
        "      <td>-0.678454</td>\n",
        "      <td>-1.094457</td>\n",
        "      <td>-0.779034</td>\n",
        "      <td>-0.578577</td>\n",
        "      <td>-0.603663</td>\n",
        "      <td>0.046115</td>\n",
        "      <td>1.202926</td>\n",
        "      <td>-0.573762</td>\n",
        "      <td>-0.044217</td>\n",
        "      <td>1.538377</td>\n",
        "      <td>-0.09914</td>\n",
        "      <td>0.911021</td>\n",
        "      <td>2.880908</td>\n",
        "      <td>-0.244400</td>\n",
        "      <td>-0.160350</td>\n",
        "      <td>-0.162999</td>\n",
        "      <td>-0.173105</td>\n",
        "      <td>-1.243063</td>\n",
        "      <td>-0.238164</td>\n",
        "      <td>-0.938330</td>\n",
        "      <td>8.790417</td>\n",
        "      <td>-0.588026</td>\n",
        "      <td>-0.612319</td>\n",
        "      <td>-0.237019</td>\n",
        "      <td>-0.229523</td>\n",
        "      <td>0.139778</td>\n",
        "      <td>3.414797</td>\n",
        "      <td>-0.164580</td>\n",
        "      <td>0.235840</td>\n",
        "      <td>2.647797</td>\n",
        "      <td>-0.511281</td>\n",
        "      <td>-0.726795</td>\n",
        "      <td>-0.761227</td>\n",
        "      <td>-0.681366</td>\n",
        "      <td>2.631027</td>\n",
        "      <td>-0.218817</td>\n",
        "      <td>-0.237575</td>\n",
        "      <td>-1.088896</td>\n",
        "      <td>0.0</td>\n",
        "      <td>-0.465301</td>\n",
        "      <td>-0.538331</td>\n",
        "      <td>-0.241267</td>\n",
        "      <td>-0.407168</td>\n",
        "      <td>0.0</td>\n",
        "      <td>-0.576629</td>\n",
        "      <td>-0.684404</td>\n",
        "      <td>-0.457252</td>\n",
        "      <td>-0.603681</td>\n",
        "      <td>-0.991985</td>\n",
        "      <td>-0.151661</td>\n",
        "      <td>-0.187524</td>\n",
        "      <td>-0.338263</td>\n",
        "      <td>-0.198922</td>\n",
        "      <td>-0.214552</td>\n",
        "      <td>-0.367138</td>\n",
        "      <td>-0.153082</td>\n",
        "      <td>-0.205322</td>\n",
        "      <td>-0.071856</td>\n",
        "      <td>-0.301045</td>\n",
        "      <td>-0.351399</td>\n",
        "      <td>-0.395999</td>\n",
        "      <td>-0.181818</td>\n",
        "      <td>-0.120873</td>\n",
        "      <td>0.0</td>\n",
        "      <td>-0.033185</td>\n",
        "      <td>-0.184258</td>\n",
        "      <td>-0.254026</td>\n",
        "      <td>-0.198101</td>\n",
        "      <td>-0.234874</td>\n",
        "      <td>0.195657</td>\n",
        "      <td>0.749980</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>108463</th>\n",
        "      <td>2.478168</td>\n",
        "      <td>-0.195281</td>\n",
        "      <td>0.059910</td>\n",
        "      <td>-1.230203</td>\n",
        "      <td>0.909935</td>\n",
        "      <td>-0.531233</td>\n",
        "      <td>-0.779034</td>\n",
        "      <td>-0.578577</td>\n",
        "      <td>-0.603663</td>\n",
        "      <td>-0.192987</td>\n",
        "      <td>-0.180543</td>\n",
        "      <td>-0.573762</td>\n",
        "      <td>-0.044217</td>\n",
        "      <td>-0.508426</td>\n",
        "      <td>-0.09914</td>\n",
        "      <td>-0.433263</td>\n",
        "      <td>-0.477368</td>\n",
        "      <td>-0.482231</td>\n",
        "      <td>-0.408521</td>\n",
        "      <td>-0.162999</td>\n",
        "      <td>-0.173105</td>\n",
        "      <td>-1.430727</td>\n",
        "      <td>-0.238164</td>\n",
        "      <td>-1.012671</td>\n",
        "      <td>-0.262700</td>\n",
        "      <td>-0.642301</td>\n",
        "      <td>-0.667505</td>\n",
        "      <td>-0.237019</td>\n",
        "      <td>-0.229523</td>\n",
        "      <td>-0.369672</td>\n",
        "      <td>-0.465324</td>\n",
        "      <td>-0.393902</td>\n",
        "      <td>-0.436011</td>\n",
        "      <td>-0.219719</td>\n",
        "      <td>-0.675058</td>\n",
        "      <td>-0.921361</td>\n",
        "      <td>-0.890554</td>\n",
        "      <td>-0.749782</td>\n",
        "      <td>-0.473218</td>\n",
        "      <td>-0.218817</td>\n",
        "      <td>-0.237575</td>\n",
        "      <td>-1.088896</td>\n",
        "      <td>0.0</td>\n",
        "      <td>-0.465301</td>\n",
        "      <td>-0.538331</td>\n",
        "      <td>-0.241267</td>\n",
        "      <td>-0.407168</td>\n",
        "      <td>0.0</td>\n",
        "      <td>-0.576629</td>\n",
        "      <td>-0.684404</td>\n",
        "      <td>-0.457252</td>\n",
        "      <td>-0.603681</td>\n",
        "      <td>-0.991985</td>\n",
        "      <td>-0.151661</td>\n",
        "      <td>-0.187524</td>\n",
        "      <td>-0.338263</td>\n",
        "      <td>-0.198922</td>\n",
        "      <td>-0.214552</td>\n",
        "      <td>-0.367138</td>\n",
        "      <td>-0.153082</td>\n",
        "      <td>-0.569749</td>\n",
        "      <td>-0.071856</td>\n",
        "      <td>-0.301045</td>\n",
        "      <td>-0.351399</td>\n",
        "      <td>-0.395999</td>\n",
        "      <td>-0.181818</td>\n",
        "      <td>-0.120873</td>\n",
        "      <td>0.0</td>\n",
        "      <td>-0.033185</td>\n",
        "      <td>-0.184258</td>\n",
        "      <td>-0.503398</td>\n",
        "      <td>-0.198101</td>\n",
        "      <td>-0.234874</td>\n",
        "      <td>-0.493332</td>\n",
        "      <td>-0.570177</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "              C6        C8       C13       C15       C16       C20       C24  \\\n",
        "93641   0.821162  1.774782  1.881226 -1.162200  2.498323 -1.094457 -0.779034   \n",
        "48047   0.388899 -0.578349  0.905957 -0.278162 -0.148991  0.595215  2.081907   \n",
        "104122 -0.403582  1.323309 -0.442353 -0.890188  1.439397 -1.094457  0.174613   \n",
        "4425   -0.979932 -1.371847  0.174504  1.863928 -0.678454 -1.094457 -0.779034   \n",
        "108463  2.478168 -0.195281  0.059910 -1.230203  0.909935 -0.531233 -0.779034   \n",
        "\n",
        "             C33       C34       C35       C37       C38       C39       C40  \\\n",
        "93641  -0.578577 -0.603663 -0.033586 -0.180543  0.322881 -0.044217 -0.508426   \n",
        "48047   2.635992  1.961841 -0.192987 -0.180543 -0.573762 -0.044217 -0.508426   \n",
        "104122 -0.578577 -0.603663 -0.113287 -0.180543  0.322881 -0.044217 -0.508426   \n",
        "4425   -0.578577 -0.603663  0.046115  1.202926 -0.573762 -0.044217  1.538377   \n",
        "108463 -0.578577 -0.603663 -0.192987 -0.180543 -0.573762 -0.044217 -0.508426   \n",
        "\n",
        "            C42       C43       C48       C51       C55       C56       C57  \\\n",
        "93641  -0.09914 -0.433263  1.629813  0.402607  0.606868  0.361610 -0.173105   \n",
        "48047  -0.09914 -0.433263 -0.059265  0.606385  0.697688  0.190255  0.782200   \n",
        "104122 -0.09914 -0.433263  0.256287  1.549600  0.105246 -0.162999 -0.173105   \n",
        "4425   -0.09914  0.911021  2.880908 -0.244400 -0.160350 -0.162999 -0.173105   \n",
        "108463 -0.09914 -0.433263 -0.477368 -0.482231 -0.408521 -0.162999 -0.173105   \n",
        "\n",
        "             C58       C59       C60       C63       C64       C65       C66  \\\n",
        "93641   1.071462 -0.238164 -0.640967 -0.262700 -0.479476 -0.336385 -0.098944   \n",
        "48047  -0.523683 -0.238164  1.366237 -0.262700  0.334649  1.208843  0.315280   \n",
        "104122 -0.773902 -0.238164  1.552089 -0.262700  2.614200 -0.170825 -0.237019   \n",
        "4425   -1.243063 -0.238164 -0.938330  8.790417 -0.588026 -0.612319 -0.237019   \n",
        "108463 -1.430727 -0.238164 -1.012671 -0.262700 -0.642301 -0.667505 -0.237019   \n",
        "\n",
        "             C67       C72       C73       C75       C76       C80       C84  \\\n",
        "93641  -0.229523  3.271858  2.544756  1.655106  1.822882 -0.219719  0.880828   \n",
        "48047   1.108718  0.212730 -0.165096 -0.104519  0.085750 -0.219719  0.553273   \n",
        "104122 -0.229523 -0.172093  1.285145  0.104948 -0.056346 -0.219719 -0.429392   \n",
        "4425   -0.229523  0.139778  3.414797 -0.164580  0.235840  2.647797 -0.511281   \n",
        "108463 -0.229523 -0.369672 -0.465324 -0.393902 -0.436011 -0.219719 -0.675058   \n",
        "\n",
        "             C88       C89       C90       C91       C98       C99      C100  \\\n",
        "93641   0.116325  0.273392  0.002798 -0.473218 -0.218817 -0.237575 -0.096930   \n",
        "48047   0.310891  0.144064 -0.407700 -0.146455 -0.218817 -0.237575 -0.152039   \n",
        "104122 -0.402518  0.208728  0.413297  0.098617 -0.218817 -0.237575 -0.034932   \n",
        "4425   -0.726795 -0.761227 -0.681366  2.631027 -0.218817 -0.237575 -1.088896   \n",
        "108463 -0.921361 -0.890554 -0.749782 -0.473218 -0.218817 -0.237575 -1.088896   \n",
        "\n",
        "        C102      C103      C104      C106      C107  C110      C111  \\\n",
        "93641    0.0  0.306521 -0.026534 -0.241267 -0.270795   0.0  1.083393   \n",
        "48047    0.0 -0.116993 -0.038301 -0.037996 -0.407168   0.0 -0.576629   \n",
        "104122   0.0 -0.310608  0.055478 -0.241267 -0.407168   0.0  1.083393   \n",
        "4425     0.0 -0.465301 -0.538331 -0.241267 -0.407168   0.0 -0.576629   \n",
        "108463   0.0 -0.465301 -0.538331 -0.241267 -0.407168   0.0 -0.576629   \n",
        "\n",
        "            C113      C114      C116      C120      C123      C124      C129  \\\n",
        "93641   0.199553 -0.079576  0.615286  0.902614 -0.151347 -0.186144  0.018421   \n",
        "48047  -0.463414 -0.079576  1.102873  1.308600 -0.151486 -0.186296 -0.325857   \n",
        "104122  1.083510 -0.079576 -0.603681  1.849914 -0.151661 -0.187524 -0.338263   \n",
        "4425   -0.684404 -0.457252 -0.603681 -0.991985 -0.151661 -0.187524 -0.338263   \n",
        "108463 -0.684404 -0.457252 -0.603681 -0.991985 -0.151661 -0.187524 -0.338263   \n",
        "\n",
        "            C138      C139      C141      C146      C157      C161      C164  \\\n",
        "93641  -0.188252 -0.214552  0.008912 -0.150861  0.523532 -0.071856 -0.301045   \n",
        "48047  -0.193587 -0.198017 -0.214294 -0.147224  2.183700 -0.071856 -0.301045   \n",
        "104122 -0.198922 -0.214552 -0.367138 -0.153082  1.454846 -0.071856 -0.301045   \n",
        "4425   -0.198922 -0.214552 -0.367138 -0.153082 -0.205322 -0.071856 -0.301045   \n",
        "108463 -0.198922 -0.214552 -0.367138 -0.153082 -0.569749 -0.071856 -0.301045   \n",
        "\n",
        "            C170      C171      C172      C174  C176      C192      C196  \\\n",
        "93641   1.823102 -0.395999 -0.181818 -0.120873   0.0 -0.033185 -0.184258   \n",
        "48047   0.735851  1.137692 -0.181818 -0.120873   0.0 -0.033185 -0.184258   \n",
        "104122 -0.351399 -0.395999 -0.181818 -0.120873   0.0 -0.033185 -0.184258   \n",
        "4425   -0.351399 -0.395999 -0.181818 -0.120873   0.0 -0.033185 -0.184258   \n",
        "108463 -0.351399 -0.395999 -0.181818 -0.120873   0.0 -0.033185 -0.184258   \n",
        "\n",
        "            C201      C202      C203      C211      C213  \n",
        "93641   0.078469  1.921761 -0.234874  0.078501 -0.043174  \n",
        "48047   3.237174  0.861830  1.920218  0.535968 -0.570177  \n",
        "104122  0.078469 -0.198101 -0.234874 -0.493332 -0.570177  \n",
        "4425   -0.254026 -0.198101 -0.234874  0.195657  0.749980  \n",
        "108463 -0.503398 -0.198101 -0.234874 -0.493332 -0.570177  "
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Feature Selection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_selection import SelectKBest, f_classif\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn import svm\n",
      "\n",
      "#coding\n",
      "X, y = dfc, df['C3'].replace({'N':0, 'Y':1})\n",
      "anova_svm = Pipeline([('anova', SelectKBest(f_classif, k=9)), ('svc', svm.SVC(kernel='linear'))])\n",
      "anova_svm.set_params(svc__C=0.1).fit(X, y)\n",
      "\n",
      "print anova_svm.score(X, y)\n",
      "\n",
      "filtered = [dfc.columns[i] for i in anova_svm.named_steps['anova'].get_support(indices=True)]\n",
      "print filtered\n",
      "\n",
      "#ranked_features = sorted(enumerate(anova_svm.named_steps['anova'].scores_), key=lambda x:x[1], reverse=True)\n",
      "#print [dfc.columns[e[0]] for e in ranked_features]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.0\n",
        "['C6', 'C8', 'C13', 'C16', 'C24', 'C33', 'C34', 'C60', 'C100']\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Python27\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [42 47 67] are constant.\n",
        "  UserWarning)\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_selection import SelectKBest, chi2\n",
      "\n",
      "dfc = df.loc[:, [c for c in df.select_dtypes(include=['object']).columns if c!='C3']]\n",
      "for col in dfc.columns:\n",
      "    dfc[col] = df[col].replace(encoder[col])\n",
      "\n",
      "selector = SelectKBest(chi2, k=9).fit(dfc, y)\n",
      "filtered2 = [dfc.columns[i] for i in selector.get_support(indices=True)]\n",
      "print filtered2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['C4', 'C9', 'C10', 'C11', 'C12', 'C22', 'C31', 'C93', 'C208']\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Preprocessing (for Prediction)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "choosed = filtered + filtered2\n",
      "choosed.remove('C22')\n",
      "choosed.remove('C24')\n",
      "\n",
      "X = df.loc[:, choosed]\n",
      "y = df['C3'].replace({'N':0, 'Y':1})\n",
      "for col in X.columns:\n",
      "  try:\n",
      "    X[col].fillna(0, inplace=True)\n",
      "    X[col] = StandardScaler().fit_transform(X[col])\n",
      "  except:\n",
      "    X[col] = X[col].replace(encoder[col])\n",
      "\n",
      "X.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>C6</th>\n",
        "      <th>C8</th>\n",
        "      <th>C13</th>\n",
        "      <th>C16</th>\n",
        "      <th>C33</th>\n",
        "      <th>C34</th>\n",
        "      <th>C60</th>\n",
        "      <th>C100</th>\n",
        "      <th>C4</th>\n",
        "      <th>C9</th>\n",
        "      <th>C10</th>\n",
        "      <th>C11</th>\n",
        "      <th>C12</th>\n",
        "      <th>C31</th>\n",
        "      <th>C93</th>\n",
        "      <th>C208</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>93641</th>\n",
        "      <td>0.821162</td>\n",
        "      <td>1.774782</td>\n",
        "      <td>1.881226</td>\n",
        "      <td>2.498323</td>\n",
        "      <td>-0.578577</td>\n",
        "      <td>-0.603663</td>\n",
        "      <td>-0.640967</td>\n",
        "      <td>-0.096930</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>48047</th>\n",
        "      <td>0.388899</td>\n",
        "      <td>-0.578349</td>\n",
        "      <td>0.905957</td>\n",
        "      <td>-0.148991</td>\n",
        "      <td>2.635992</td>\n",
        "      <td>1.961841</td>\n",
        "      <td>1.366237</td>\n",
        "      <td>-0.152039</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>104122</th>\n",
        "      <td>-0.403582</td>\n",
        "      <td>1.323309</td>\n",
        "      <td>-0.442353</td>\n",
        "      <td>1.439397</td>\n",
        "      <td>-0.578577</td>\n",
        "      <td>-0.603663</td>\n",
        "      <td>1.552089</td>\n",
        "      <td>-0.034932</td>\n",
        "      <td>0</td>\n",
        "      <td>2</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>2</td>\n",
        "      <td>2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4425</th>\n",
        "      <td>-0.979932</td>\n",
        "      <td>-1.371847</td>\n",
        "      <td>0.174504</td>\n",
        "      <td>-0.678454</td>\n",
        "      <td>-0.578577</td>\n",
        "      <td>-0.603663</td>\n",
        "      <td>-0.938330</td>\n",
        "      <td>-1.088896</td>\n",
        "      <td>0</td>\n",
        "      <td>3</td>\n",
        "      <td>1</td>\n",
        "      <td>2</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>108463</th>\n",
        "      <td>2.478168</td>\n",
        "      <td>-0.195281</td>\n",
        "      <td>0.059910</td>\n",
        "      <td>0.909935</td>\n",
        "      <td>-0.578577</td>\n",
        "      <td>-0.603663</td>\n",
        "      <td>-1.012671</td>\n",
        "      <td>-1.088896</td>\n",
        "      <td>0</td>\n",
        "      <td>2</td>\n",
        "      <td>0</td>\n",
        "      <td>3</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "              C6        C8       C13       C16       C33       C34       C60  \\\n",
        "93641   0.821162  1.774782  1.881226  2.498323 -0.578577 -0.603663 -0.640967   \n",
        "48047   0.388899 -0.578349  0.905957 -0.148991  2.635992  1.961841  1.366237   \n",
        "104122 -0.403582  1.323309 -0.442353  1.439397 -0.578577 -0.603663  1.552089   \n",
        "4425   -0.979932 -1.371847  0.174504 -0.678454 -0.578577 -0.603663 -0.938330   \n",
        "108463  2.478168 -0.195281  0.059910  0.909935 -0.578577 -0.603663 -1.012671   \n",
        "\n",
        "            C100  C4  C9  C10  C11  C12  C31  C93  C208  \n",
        "93641  -0.096930   0   0    0    0    0    0    0     0  \n",
        "48047  -0.152039   0   1    1    1    0    1    1     1  \n",
        "104122 -0.034932   0   2    0    0    1    0    2     2  \n",
        "4425   -1.088896   0   3    1    2    0    0    0     0  \n",
        "108463 -1.088896   0   2    0    3    1    0    0     0  "
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Decision Tree Classifier (CART)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "from sklearn import tree\n",
      "\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4, random_state=0)\n",
      "\n",
      "clf0 = tree.DecisionTreeClassifier().fit(X_train, y_train) #CART\n",
      "clf0.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "0.57225000000000004"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_code(tree, feature_names):\n",
      "  left = tree.tree_.children_left\n",
      "  right = tree.tree_.children_right\n",
      "  threshold = tree.tree_.threshold\n",
      "  features = [feature_names[i] for i in tree.tree_.feature]\n",
      "  value = tree.tree_.value\n",
      "\n",
      "  def recurse(left, right, threshold, features, node):\n",
      "    if threshold[node] != -2:\n",
      "      print \"if ( \" + features[node] + \" <= \" + str(threshold[node]) + \" ) {\"\n",
      "      if left[node] != -1:\n",
      "        recurse (left, right, threshold, features,left[node])\n",
      "      print \"} else {\"\n",
      "      if right[node] != -1:\n",
      "        recurse (left, right, threshold, features,right[node])\n",
      "      print \"}\"\n",
      "    else:\n",
      "      print \"return \" + str(value[node])\n",
      "  \n",
      "  recurse(left, right, threshold, features, 0)\n",
      "\n",
      "#get_code(clf, X.columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Naive Bayes Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import GaussianNB\n",
      "\n",
      "#X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4, random_state=0)\n",
      "\n",
      "clf1 = GaussianNB().fit(X_train, y_train)\n",
      "clf1.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "0.51075000000000004"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Logistic Regression (aka logit, MaxEnt) Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "#X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4, random_state=0)\n",
      "\n",
      "clf2 = LogisticRegression().fit(X_train, y_train)\n",
      "clf2.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "0.63375000000000004"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Random Forest Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "#X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4, random_state=0)\n",
      "\n",
      "clf3 = RandomForestClassifier(n_estimators=100, class_weight={0:1, 1:10}).fit(X_train, y_train)\n",
      "print clf3.score(X_test, y_test)\n",
      "\n",
      "print sorted(zip(choosed, clf3.feature_importances_), key=lambda x: x[1], reverse=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.66775\n",
        "[('C8', 0.15168253904269707), ('C6', 0.13327653061130648), ('C60', 0.10748498766396486), ('C93', 0.095129428508422334), ('C100', 0.092835243587478772), ('C9', 0.089235068966154532), ('C13', 0.071229955677005907), ('C16', 0.064611024839233075), ('C11', 0.053064390788912973), ('C33', 0.036928098878851491), ('C12', 0.0226886898923544), ('C208', 0.022416590697054567), ('C31', 0.016284453320004336), ('C10', 0.016073354467816197), ('C4', 0.015122323172519431), ('C34', 0.011937319886223517)]\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##SVM Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "from sklearn import svm\n",
      "\n",
      "#X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4, random_state=0)\n",
      "\n",
      "#clf4 = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
      "clf4 = svm.SVC(kernel='linear', probability=True).fit(X_train, y_train)\n",
      "clf4.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "0.63324999999999998"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Clustering and Choose Classifier for each group"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import KMeans\n",
      "\n",
      "#X_cData = X.loc[:, [c for c in filtered[:8] if c!='C24']].fillna(0)\n",
      "choosed_for_clustering = ['C8', 'C6', 'C60'] #[c for c in filtered[:8] if c!='C24']\n",
      "X_cData = X.loc[:, choosed_for_clustering]\n",
      "\n",
      "kmeans = KMeans(init='k-means++', n_clusters=3)\n",
      "kmeans.fit(X_cData)\n",
      "df['clabel'] = kmeans.labels_\n",
      "#df['clabel'].value_counts(sort=True)\n",
      "\n",
      "clfs = [None for i in xrange(3)]\n",
      "for i in xrange(3):\n",
      "    _X = df.loc[df['clabel']==i, choosed]\n",
      "    _y = df.loc[df['clabel']==i, 'C3'].replace({'N':0, 'Y':1})\n",
      "    for col in _X.columns:\n",
      "      try:\n",
      "        _X[col].fillna(0, inplace=True)\n",
      "        _X[col] = StandardScaler().fit_transform(_X[col])\n",
      "      except:\n",
      "        _X[col] = _X[col].replace(encoder[col])\n",
      "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(_X, _y, test_size=0.4, random_state=0)\n",
      "    clfs[i] = RandomForestClassifier(n_estimators=100, class_weight={0:1, 1:10}).fit(X_train, y_train)\n",
      "    score = clfs[i].score(X_test, y_test)\n",
      "    if score<0.65: clfs[i] = svm.SVC(kernel='linear', probability=True).fit(X_train, y_train)\n",
      "    print clfs[i].score(X_test, y_test)\n",
      "\n",
      "#X_train.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.60549132948\n",
        "0.586186883343"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.659955257271"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Voting Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import VotingClassifier\n",
      "\n",
      "vclf = VotingClassifier(estimators=[('naiveBayes', clf1), ('LogisticR', clf2), ('randomF', clf3), ('svc', clf4)], voting='soft', weights=[1,3,5,3]).fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Prediction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "dfp = pd.read_csv('../../MB_PCUST_NPOUT_EXAM.txt', encoding='utf-8', sep='\\t', names=['C2']+['C'+str(i+4) for i in xrange(212)], na_values=['?', '\\N'])\n",
      "\n",
      "Xp = dfp.loc[:, choosed]\n",
      "for col in Xp.columns:\n",
      "  try:\n",
      "    Xp[col].fillna(0, inplace=True)\n",
      "    Xp[col] = StandardScaler().fit_transform(Xp[col])\n",
      "  except:\n",
      "    Xp[col] = Xp[col].replace(encoder[col])\n",
      "\n",
      "dfp['pred_C3'] = vclf.predict(Xp)\n",
      "dfp['C3'] = dfp['C24'].map(lambda x: 1 if x>0 else 0)\n",
      "\n",
      "print accuracy_score(dfp['C3'], dfp['pred_C3'])\n",
      "\n",
      "dfp.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.6766\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>C2</th>\n",
        "      <th>C4</th>\n",
        "      <th>C5</th>\n",
        "      <th>C6</th>\n",
        "      <th>C7</th>\n",
        "      <th>C8</th>\n",
        "      <th>C9</th>\n",
        "      <th>C10</th>\n",
        "      <th>C11</th>\n",
        "      <th>C12</th>\n",
        "      <th>C13</th>\n",
        "      <th>C14</th>\n",
        "      <th>C15</th>\n",
        "      <th>C16</th>\n",
        "      <th>C17</th>\n",
        "      <th>C18</th>\n",
        "      <th>C19</th>\n",
        "      <th>C20</th>\n",
        "      <th>C21</th>\n",
        "      <th>C22</th>\n",
        "      <th>C23</th>\n",
        "      <th>C24</th>\n",
        "      <th>C25</th>\n",
        "      <th>C26</th>\n",
        "      <th>C27</th>\n",
        "      <th>C28</th>\n",
        "      <th>C29</th>\n",
        "      <th>C30</th>\n",
        "      <th>C31</th>\n",
        "      <th>C32</th>\n",
        "      <th>C33</th>\n",
        "      <th>C34</th>\n",
        "      <th>C35</th>\n",
        "      <th>C36</th>\n",
        "      <th>C37</th>\n",
        "      <th>C38</th>\n",
        "      <th>C39</th>\n",
        "      <th>C40</th>\n",
        "      <th>C41</th>\n",
        "      <th>C42</th>\n",
        "      <th>C43</th>\n",
        "      <th>C44</th>\n",
        "      <th>C45</th>\n",
        "      <th>C46</th>\n",
        "      <th>C47</th>\n",
        "      <th>C48</th>\n",
        "      <th>C49</th>\n",
        "      <th>C50</th>\n",
        "      <th>C51</th>\n",
        "      <th>C52</th>\n",
        "      <th>C53</th>\n",
        "      <th>C54</th>\n",
        "      <th>C55</th>\n",
        "      <th>C56</th>\n",
        "      <th>C57</th>\n",
        "      <th>C58</th>\n",
        "      <th>C59</th>\n",
        "      <th>C60</th>\n",
        "      <th>C61</th>\n",
        "      <th>C62</th>\n",
        "      <th>C63</th>\n",
        "      <th>C64</th>\n",
        "      <th>C65</th>\n",
        "      <th>C66</th>\n",
        "      <th>C67</th>\n",
        "      <th>C68</th>\n",
        "      <th>C69</th>\n",
        "      <th>C70</th>\n",
        "      <th>C71</th>\n",
        "      <th>C72</th>\n",
        "      <th>C73</th>\n",
        "      <th>C74</th>\n",
        "      <th>C75</th>\n",
        "      <th>C76</th>\n",
        "      <th>C77</th>\n",
        "      <th>C78</th>\n",
        "      <th>C79</th>\n",
        "      <th>C80</th>\n",
        "      <th>C81</th>\n",
        "      <th>C82</th>\n",
        "      <th>C83</th>\n",
        "      <th>C84</th>\n",
        "      <th>C85</th>\n",
        "      <th>C86</th>\n",
        "      <th>C87</th>\n",
        "      <th>C88</th>\n",
        "      <th>C89</th>\n",
        "      <th>C90</th>\n",
        "      <th>C91</th>\n",
        "      <th>C92</th>\n",
        "      <th>C93</th>\n",
        "      <th>C94</th>\n",
        "      <th>C95</th>\n",
        "      <th>C96</th>\n",
        "      <th>C97</th>\n",
        "      <th>C98</th>\n",
        "      <th>C99</th>\n",
        "      <th>C100</th>\n",
        "      <th>C101</th>\n",
        "      <th>C102</th>\n",
        "      <th>C103</th>\n",
        "      <th>C104</th>\n",
        "      <th>C105</th>\n",
        "      <th>C106</th>\n",
        "      <th>C107</th>\n",
        "      <th>C108</th>\n",
        "      <th>C109</th>\n",
        "      <th>C110</th>\n",
        "      <th>C111</th>\n",
        "      <th>C112</th>\n",
        "      <th>C113</th>\n",
        "      <th>C114</th>\n",
        "      <th>C115</th>\n",
        "      <th>C116</th>\n",
        "      <th>C117</th>\n",
        "      <th>C118</th>\n",
        "      <th>C119</th>\n",
        "      <th>C120</th>\n",
        "      <th>C121</th>\n",
        "      <th>C122</th>\n",
        "      <th>C123</th>\n",
        "      <th>C124</th>\n",
        "      <th>C125</th>\n",
        "      <th>C126</th>\n",
        "      <th>C127</th>\n",
        "      <th>C128</th>\n",
        "      <th>C129</th>\n",
        "      <th>C130</th>\n",
        "      <th>C131</th>\n",
        "      <th>C132</th>\n",
        "      <th>C133</th>\n",
        "      <th>C134</th>\n",
        "      <th>C135</th>\n",
        "      <th>C136</th>\n",
        "      <th>C137</th>\n",
        "      <th>C138</th>\n",
        "      <th>C139</th>\n",
        "      <th>C140</th>\n",
        "      <th>C141</th>\n",
        "      <th>C142</th>\n",
        "      <th>C143</th>\n",
        "      <th>C144</th>\n",
        "      <th>C145</th>\n",
        "      <th>C146</th>\n",
        "      <th>C147</th>\n",
        "      <th>C148</th>\n",
        "      <th>C149</th>\n",
        "      <th>C150</th>\n",
        "      <th>C151</th>\n",
        "      <th>C152</th>\n",
        "      <th>C153</th>\n",
        "      <th>C154</th>\n",
        "      <th>C155</th>\n",
        "      <th>C156</th>\n",
        "      <th>C157</th>\n",
        "      <th>C158</th>\n",
        "      <th>C159</th>\n",
        "      <th>C160</th>\n",
        "      <th>C161</th>\n",
        "      <th>C162</th>\n",
        "      <th>C163</th>\n",
        "      <th>C164</th>\n",
        "      <th>C165</th>\n",
        "      <th>C166</th>\n",
        "      <th>C167</th>\n",
        "      <th>C168</th>\n",
        "      <th>C169</th>\n",
        "      <th>C170</th>\n",
        "      <th>C171</th>\n",
        "      <th>C172</th>\n",
        "      <th>C173</th>\n",
        "      <th>C174</th>\n",
        "      <th>C175</th>\n",
        "      <th>C176</th>\n",
        "      <th>C177</th>\n",
        "      <th>C178</th>\n",
        "      <th>C179</th>\n",
        "      <th>C180</th>\n",
        "      <th>C181</th>\n",
        "      <th>C182</th>\n",
        "      <th>C183</th>\n",
        "      <th>C184</th>\n",
        "      <th>C185</th>\n",
        "      <th>C186</th>\n",
        "      <th>C187</th>\n",
        "      <th>C188</th>\n",
        "      <th>C189</th>\n",
        "      <th>C190</th>\n",
        "      <th>C191</th>\n",
        "      <th>C192</th>\n",
        "      <th>C193</th>\n",
        "      <th>C194</th>\n",
        "      <th>C195</th>\n",
        "      <th>C196</th>\n",
        "      <th>C197</th>\n",
        "      <th>C198</th>\n",
        "      <th>C199</th>\n",
        "      <th>C200</th>\n",
        "      <th>C201</th>\n",
        "      <th>C202</th>\n",
        "      <th>C203</th>\n",
        "      <th>C204</th>\n",
        "      <th>C205</th>\n",
        "      <th>C206</th>\n",
        "      <th>C207</th>\n",
        "      <th>C208</th>\n",
        "      <th>C209</th>\n",
        "      <th>C210</th>\n",
        "      <th>C211</th>\n",
        "      <th>C212</th>\n",
        "      <th>C213</th>\n",
        "      <th>C214</th>\n",
        "      <th>C215</th>\n",
        "      <th>pred_C3</th>\n",
        "      <th>C3</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>C0088856</td>\n",
        "      <td>NVIP</td>\n",
        "      <td>M</td>\n",
        "      <td>45.0</td>\n",
        "      <td>\u96d9\u9b5a\u5ea7</td>\n",
        "      <td>14</td>\n",
        "      <td>\u64da\u9ede02</td>\n",
        "      <td>N</td>\n",
        "      <td>\u6975\u4f4e</td>\n",
        "      <td>3G</td>\n",
        "      <td>183.0</td>\n",
        "      <td>\u7121\u7d81\u7d04</td>\n",
        "      <td>NaN</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>\u96f6\u5143</td>\n",
        "      <td>\u96f6\u5143</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>\u7121</td>\n",
        "      <td>N</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>\u7121NP</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>6</td>\n",
        "      <td>3</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>2</td>\n",
        "      <td>NaN</td>\n",
        "      <td>Yes</td>\n",
        "      <td>No</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>No</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>C0268970</td>\n",
        "      <td>NVIP</td>\n",
        "      <td>F</td>\n",
        "      <td>38.0</td>\n",
        "      <td>\u7345\u5b50\u5ea7</td>\n",
        "      <td>174</td>\n",
        "      <td>\u64da\u9ede07</td>\n",
        "      <td>N</td>\n",
        "      <td>\u4e2d</td>\n",
        "      <td>4G</td>\n",
        "      <td>1136.0</td>\n",
        "      <td>\u8cfc\u6a5f\u7d04</td>\n",
        "      <td>-70.0</td>\n",
        "      <td>8</td>\n",
        "      <td>6</td>\n",
        "      <td>\u4e2d</td>\n",
        "      <td>\u9ad8</td>\n",
        "      <td>1.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>\u7121</td>\n",
        "      <td>N</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>\u7121NP</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>4</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>2</td>\n",
        "      <td>Yes</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>5.165101e+06</td>\n",
        "      <td>99.34</td>\n",
        "      <td>77.32</td>\n",
        "      <td>0.0</td>\n",
        "      <td>7.21</td>\n",
        "      <td>12.93</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>2.93</td>\n",
        "      <td>0.00</td>\n",
        "      <td>4.28</td>\n",
        "      <td>0.78</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.07</td>\n",
        "      <td>0.13</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.02</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.03</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.04</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>3.57</td>\n",
        "      <td>1.12</td>\n",
        "      <td>3.95</td>\n",
        "      <td>5.38</td>\n",
        "      <td>0.78</td>\n",
        "      <td>4.22</td>\n",
        "      <td>20.43</td>\n",
        "      <td>59.15</td>\n",
        "      <td>0.75</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.04</td>\n",
        "      <td>0.01</td>\n",
        "      <td>0.04</td>\n",
        "      <td>0.05</td>\n",
        "      <td>0.01</td>\n",
        "      <td>0.04</td>\n",
        "      <td>0.21</td>\n",
        "      <td>0.60</td>\n",
        "      <td>0.01</td>\n",
        "      <td>\u53f0\u5357\u5e02</td>\n",
        "      <td>\u65b0\u71df\u5340</td>\n",
        "      <td>\u53f0\u5357\u5e02</td>\n",
        "      <td>\u65b0\u71df\u5340</td>\n",
        "      <td>No</td>\n",
        "      <td>No</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>132.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000</td>\n",
        "      <td>0.000</td>\n",
        "      <td>0.000</td>\n",
        "      <td>0.000000</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>6.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>108701933.0</td>\n",
        "      <td>151396249.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>640602330.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>113758047.0</td>\n",
        "      <td>85067978.0</td>\n",
        "      <td>135352031.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>51434728.0</td>\n",
        "      <td>1.795074e+09</td>\n",
        "      <td>439151.0</td>\n",
        "      <td>326260868.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.906637e+09</td>\n",
        "      <td>0.0</td>\n",
        "      <td>557196876.0</td>\n",
        "      <td>5.169043e+09</td>\n",
        "      <td>448633971.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.716211e+09</td>\n",
        "      <td>570153410.0</td>\n",
        "      <td>99.35</td>\n",
        "      <td>77.32</td>\n",
        "      <td>7.21</td>\n",
        "      <td>12.93</td>\n",
        "      <td>12.93</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>79.21</td>\n",
        "      <td>59.0</td>\n",
        "      <td>24.0</td>\n",
        "      <td>19.0</td>\n",
        "      <td>13.0</td>\n",
        "      <td>13.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>13.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>6.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>13.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>6.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>90.26</td>\n",
        "      <td>77.32</td>\n",
        "      <td>0.0</td>\n",
        "      <td>12.94</td>\n",
        "      <td>0.86</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.14</td>\n",
        "      <td>59.0</td>\n",
        "      <td>24.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>19.0</td>\n",
        "      <td>13.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>17.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>37.0</td>\n",
        "      <td>24.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>13.0</td>\n",
        "      <td>\u696d\u8005</td>\n",
        "      <td>9.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>232.0</td>\n",
        "      <td>903739.278320</td>\n",
        "      <td>0.174970</td>\n",
        "      <td>232.0</td>\n",
        "      <td>5.289064e+09</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>C0298512</td>\n",
        "      <td>NVIP</td>\n",
        "      <td>M</td>\n",
        "      <td>62.0</td>\n",
        "      <td>\u5c04\u624b\u5ea7</td>\n",
        "      <td>73</td>\n",
        "      <td>\u64da\u9ede07</td>\n",
        "      <td>N</td>\n",
        "      <td>\u6975\u4f4e</td>\n",
        "      <td>3G</td>\n",
        "      <td>183.0</td>\n",
        "      <td>\u901a\u4fe1\u7d04</td>\n",
        "      <td>-63.0</td>\n",
        "      <td>2</td>\n",
        "      <td>3</td>\n",
        "      <td>\u4f4e\u4e2d</td>\n",
        "      <td>\u4f4e\u4e2d</td>\n",
        "      <td>2.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>\u7af6\u696dB</td>\n",
        "      <td>N</td>\n",
        "      <td>2</td>\n",
        "      <td>1.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>NaN</td>\n",
        "      <td>1</td>\n",
        "      <td>NaN</td>\n",
        "      <td>1.0</td>\n",
        "      <td>\u7af6\u696dA</td>\n",
        "      <td>72.0</td>\n",
        "      <td>72.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>3</td>\n",
        "      <td>2</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>2</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>NaN</td>\n",
        "      <td>25.49</td>\n",
        "      <td>6.35</td>\n",
        "      <td>0.0</td>\n",
        "      <td>19.14</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>5.62</td>\n",
        "      <td>13.52</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.25</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.75</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.22</td>\n",
        "      <td>0.53</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>1.53</td>\n",
        "      <td>8.97</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.38</td>\n",
        "      <td>0.00</td>\n",
        "      <td>6.05</td>\n",
        "      <td>8.55</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.06</td>\n",
        "      <td>0.35</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.01</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.24</td>\n",
        "      <td>0.34</td>\n",
        "      <td>0.00</td>\n",
        "      <td>\u53f0\u5357\u5e02</td>\n",
        "      <td>\u67f3\u71df\u5340</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>No</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>538.0</td>\n",
        "      <td>179.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>21.645</td>\n",
        "      <td>3.652</td>\n",
        "      <td>7.830</td>\n",
        "      <td>0.000000</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>25.48</td>\n",
        "      <td>6.35</td>\n",
        "      <td>19.14</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>13.52</td>\n",
        "      <td>16.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>12.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>9.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>6.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>9.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>6.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>6.36</td>\n",
        "      <td>6.36</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>1.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>16.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>12.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>9.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>\u7af6\u696dB</td>\n",
        "      <td>3.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>C0249782</td>\n",
        "      <td>NVIP</td>\n",
        "      <td>M</td>\n",
        "      <td>56.0</td>\n",
        "      <td>\u91d1\u725b\u5ea7</td>\n",
        "      <td>226</td>\n",
        "      <td>\u64da\u9ede07</td>\n",
        "      <td>N</td>\n",
        "      <td>\u6975\u4f4e</td>\n",
        "      <td>3G</td>\n",
        "      <td>183.0</td>\n",
        "      <td>\u8cfc\u6a5f\u7d04</td>\n",
        "      <td>-77.0</td>\n",
        "      <td>4</td>\n",
        "      <td>2</td>\n",
        "      <td>\u4f4e</td>\n",
        "      <td>\u96f6\u5143</td>\n",
        "      <td>2.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>\u7121</td>\n",
        "      <td>N</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>\u7121NP</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>8</td>\n",
        "      <td>3</td>\n",
        "      <td>1</td>\n",
        "      <td>2</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>Yes</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>NaN</td>\n",
        "      <td>21.43</td>\n",
        "      <td>5.22</td>\n",
        "      <td>0.0</td>\n",
        "      <td>13.64</td>\n",
        "      <td>2.57</td>\n",
        "      <td>0.0</td>\n",
        "      <td>4.07</td>\n",
        "      <td>9.57</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.24</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.64</td>\n",
        "      <td>0.12</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.19</td>\n",
        "      <td>0.45</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.40</td>\n",
        "      <td>2.57</td>\n",
        "      <td>7.13</td>\n",
        "      <td>0.22</td>\n",
        "      <td>0.00</td>\n",
        "      <td>9.17</td>\n",
        "      <td>2.87</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.02</td>\n",
        "      <td>0.11</td>\n",
        "      <td>0.32</td>\n",
        "      <td>0.01</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.41</td>\n",
        "      <td>0.13</td>\n",
        "      <td>0.00</td>\n",
        "      <td>\u53f0\u5357\u5e02</td>\n",
        "      <td>\u65b0\u71df\u5340</td>\n",
        "      <td>\u53f0\u5357\u5e02</td>\n",
        "      <td>\u67f3\u71df\u5340</td>\n",
        "      <td>No</td>\n",
        "      <td>Yes</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>244.0</td>\n",
        "      <td>92.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>45.672</td>\n",
        "      <td>33.880</td>\n",
        "      <td>5.709</td>\n",
        "      <td>5.704305</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>22.35</td>\n",
        "      <td>6.14</td>\n",
        "      <td>13.64</td>\n",
        "      <td>2.57</td>\n",
        "      <td>2.57</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>6.14</td>\n",
        "      <td>16.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>6.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>9.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>10.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>8.70</td>\n",
        "      <td>6.14</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2.56</td>\n",
        "      <td>0.71</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.29</td>\n",
        "      <td>14.0</td>\n",
        "      <td>6.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>6.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>10.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>\u696d\u8005</td>\n",
        "      <td>7.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>C0180972</td>\n",
        "      <td>NVIP</td>\n",
        "      <td>M</td>\n",
        "      <td>46.0</td>\n",
        "      <td>\u9b54\u7faf\u5ea7</td>\n",
        "      <td>83</td>\n",
        "      <td>\u64da\u9ede04</td>\n",
        "      <td>N</td>\n",
        "      <td>\u4e2d</td>\n",
        "      <td>3G</td>\n",
        "      <td>383.0</td>\n",
        "      <td>\u901a\u4fe1\u7d04</td>\n",
        "      <td>-73.0</td>\n",
        "      <td>2</td>\n",
        "      <td>2</td>\n",
        "      <td>\u4f4e</td>\n",
        "      <td>\u4f4e\u4e2d</td>\n",
        "      <td>3.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>\u7121</td>\n",
        "      <td>N</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>\u7af6\u696dD</td>\n",
        "      <td>82.0</td>\n",
        "      <td>106.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>9.011243e+05</td>\n",
        "      <td>58.42</td>\n",
        "      <td>14.80</td>\n",
        "      <td>0.0</td>\n",
        "      <td>17.35</td>\n",
        "      <td>26.27</td>\n",
        "      <td>0.0</td>\n",
        "      <td>6.35</td>\n",
        "      <td>0.62</td>\n",
        "      <td>9.88</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.25</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.30</td>\n",
        "      <td>0.45</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.11</td>\n",
        "      <td>0.01</td>\n",
        "      <td>0.17</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>8.97</td>\n",
        "      <td>14.07</td>\n",
        "      <td>8.05</td>\n",
        "      <td>23.27</td>\n",
        "      <td>3.23</td>\n",
        "      <td>0.83</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.15</td>\n",
        "      <td>0.24</td>\n",
        "      <td>0.14</td>\n",
        "      <td>0.40</td>\n",
        "      <td>0.06</td>\n",
        "      <td>0.01</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>\u5f70\u5316\u7e23</td>\n",
        "      <td>\u548c\u7f8e\u93ae</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>No</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>93.0</td>\n",
        "      <td>72.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>45.726</td>\n",
        "      <td>46.141</td>\n",
        "      <td>28.992</td>\n",
        "      <td>0.000000</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>6.0</td>\n",
        "      <td>13.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>562953.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>6514284.0</td>\n",
        "      <td>45907574.0</td>\n",
        "      <td>581620.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.128205e+06</td>\n",
        "      <td>0.0</td>\n",
        "      <td>567134600.0</td>\n",
        "      <td>2.739706e+08</td>\n",
        "      <td>21208770.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>58.42</td>\n",
        "      <td>14.80</td>\n",
        "      <td>16.85</td>\n",
        "      <td>26.27</td>\n",
        "      <td>26.27</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>26.27</td>\n",
        "      <td>51.0</td>\n",
        "      <td>12.0</td>\n",
        "      <td>10.0</td>\n",
        "      <td>28.0</td>\n",
        "      <td>28.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>27.0</td>\n",
        "      <td>6.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>13.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>27.0</td>\n",
        "      <td>6.0</td>\n",
        "      <td>7.0</td>\n",
        "      <td>13.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>41.07</td>\n",
        "      <td>14.80</td>\n",
        "      <td>0.0</td>\n",
        "      <td>26.27</td>\n",
        "      <td>0.36</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.64</td>\n",
        "      <td>51.0</td>\n",
        "      <td>12.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>11.0</td>\n",
        "      <td>28.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>6.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>40.0</td>\n",
        "      <td>12.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>28.0</td>\n",
        "      <td>\u56fa\u7db2</td>\n",
        "      <td>24.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>259.0</td>\n",
        "      <td>57456.744141</td>\n",
        "      <td>0.063761</td>\n",
        "      <td>259.0</td>\n",
        "      <td>9.227513e+08</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "         C2    C4 C5    C6   C7   C8    C9 C10 C11 C12     C13  C14   C15  \\\n",
        "0  C0088856  NVIP  M  45.0  \u96d9\u9b5a\u5ea7   14  \u64da\u9ede02   N  \u6975\u4f4e  3G   183.0  \u7121\u7d81\u7d04   NaN   \n",
        "1  C0268970  NVIP  F  38.0  \u7345\u5b50\u5ea7  174  \u64da\u9ede07   N   \u4e2d  4G  1136.0  \u8cfc\u6a5f\u7d04 -70.0   \n",
        "2  C0298512  NVIP  M  62.0  \u5c04\u624b\u5ea7   73  \u64da\u9ede07   N  \u6975\u4f4e  3G   183.0  \u901a\u4fe1\u7d04 -63.0   \n",
        "3  C0249782  NVIP  M  56.0  \u91d1\u725b\u5ea7  226  \u64da\u9ede07   N  \u6975\u4f4e  3G   183.0  \u8cfc\u6a5f\u7d04 -77.0   \n",
        "4  C0180972  NVIP  M  46.0  \u9b54\u7faf\u5ea7   83  \u64da\u9ede04   N   \u4e2d  3G   383.0  \u901a\u4fe1\u7d04 -73.0   \n",
        "\n",
        "   C16  C17 C18 C19  C20  C21  C22 C23  C24  C25  C26  C27  C28  C29  C30  \\\n",
        "0    0    0  \u96f6\u5143  \u96f6\u5143  NaN  NaN    \u7121   N    0  0.0  0.0  0.0    0  0.0  0.0   \n",
        "1    8    6   \u4e2d   \u9ad8  1.0  4.0    \u7121   N    0  0.0  0.0  0.0    0  0.0  0.0   \n",
        "2    2    3  \u4f4e\u4e2d  \u4f4e\u4e2d  2.0  3.0  \u7af6\u696dB   N    2  1.0  1.0  NaN    1  NaN  1.0   \n",
        "3    4    2   \u4f4e  \u96f6\u5143  2.0  1.0    \u7121   N    0  0.0  0.0  0.0    0  0.0  0.0   \n",
        "4    2    2   \u4f4e  \u4f4e\u4e2d  3.0  2.0    \u7121   N    0  0.0  0.0  0.0    0  0.0  0.0   \n",
        "\n",
        "   C31   C32    C33  C34  C35  C36  C37  C38  C39  C40  C41  C42  C43  C44  \\\n",
        "0  \u7121NP   NaN    NaN  NaN    6    3    1    0    0    1    1    0    2  NaN   \n",
        "1  \u7121NP   NaN    NaN  NaN    4    1    0    1    0    1    1    0    2  Yes   \n",
        "2  \u7af6\u696dA  72.0   72.0  1.0    3    2    0    1    0    0    0    0    2  NaN   \n",
        "3  \u7121NP   NaN    NaN  NaN    8    3    1    2    0    1    1    0    1  Yes   \n",
        "4  \u7af6\u696dD  82.0  106.0  2.0    1    1    0    0    0    0    0    0    1  NaN   \n",
        "\n",
        "   C45 C46           C47    C48    C49  C50    C51    C52  C53   C54    C55  \\\n",
        "0  Yes  No           NaN    NaN    NaN  NaN    NaN    NaN  NaN   NaN    NaN   \n",
        "1  NaN  No  5.165101e+06  99.34  77.32  0.0   7.21  12.93  0.0  0.00   2.93   \n",
        "2  NaN  No           NaN  25.49   6.35  0.0  19.14   0.00  0.0  5.62  13.52   \n",
        "3  NaN  No           NaN  21.43   5.22  0.0  13.64   2.57  0.0  4.07   9.57   \n",
        "4  NaN  No  9.011243e+05  58.42  14.80  0.0  17.35  26.27  0.0  6.35   0.62   \n",
        "\n",
        "    C56   C57   C58  C59   C60   C61  C62   C63   C64   C65   C66   C67  C68  \\\n",
        "0   NaN   NaN   NaN  NaN   NaN   NaN  NaN   NaN   NaN   NaN   NaN   NaN  NaN   \n",
        "1  0.00  4.28  0.78  0.0  0.07  0.13  0.0  0.02  0.00  0.03  0.00  0.04  0.0   \n",
        "2  0.00  0.00  0.25  0.0  0.75  0.00  0.0  0.00  0.22  0.53  0.00  0.00  0.0   \n",
        "3  0.00  0.00  0.24  0.0  0.64  0.12  0.0  0.00  0.19  0.45  0.00  0.00  0.0   \n",
        "4  9.88  0.00  0.25  0.0  0.30  0.45  0.0  0.00  0.11  0.01  0.17  0.00  0.0   \n",
        "\n",
        "   C69  C70   C71   C72    C73   C74    C75   C76    C77    C78   C79  C80  \\\n",
        "0  NaN  NaN   NaN   NaN    NaN   NaN    NaN   NaN    NaN    NaN   NaN  NaN   \n",
        "1  0.0  0.0  3.57  1.12   3.95  5.38   0.78  4.22  20.43  59.15  0.75  0.0   \n",
        "2  0.0  0.0  0.00  1.53   8.97  0.00   0.38  0.00   6.05   8.55  0.00  0.0   \n",
        "3  0.0  0.0  0.00  0.40   2.57  7.13   0.22  0.00   9.17   2.87  0.00  0.0   \n",
        "4  0.0  0.0  0.00  8.97  14.07  8.05  23.27  3.23   0.83   0.00  0.00  0.0   \n",
        "\n",
        "   C81  C82   C83   C84   C85   C86   C87   C88   C89   C90   C91  C92  C93  \\\n",
        "0  NaN  NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN  NaN   \n",
        "1  0.0  0.0  0.04  0.01  0.04  0.05  0.01  0.04  0.21  0.60  0.01  \u53f0\u5357\u5e02  \u65b0\u71df\u5340   \n",
        "2  0.0  0.0  0.00  0.06  0.35  0.00  0.01  0.00  0.24  0.34  0.00  \u53f0\u5357\u5e02  \u67f3\u71df\u5340   \n",
        "3  0.0  0.0  0.00  0.02  0.11  0.32  0.01  0.00  0.41  0.13  0.00  \u53f0\u5357\u5e02  \u65b0\u71df\u5340   \n",
        "4  0.0  0.0  0.00  0.15  0.24  0.14  0.40  0.06  0.01  0.00  0.00  \u5f70\u5316\u7e23  \u548c\u7f8e\u93ae   \n",
        "\n",
        "   C94  C95 C96  C97  C98  C99   C100   C101  C102    C103    C104    C105  \\\n",
        "0  NaN  NaN  No   No  NaN  NaN    NaN    NaN   NaN     NaN     NaN     NaN   \n",
        "1  \u53f0\u5357\u5e02  \u65b0\u71df\u5340  No   No  0.0  0.0  132.0    0.0   0.0   0.000   0.000   0.000   \n",
        "2  NaN  NaN  No   No  0.0  0.0  538.0  179.0   0.0  21.645   3.652   7.830   \n",
        "3  \u53f0\u5357\u5e02  \u67f3\u71df\u5340  No  Yes  1.0  0.0  244.0   92.0   0.0  45.672  33.880   5.709   \n",
        "4  NaN  NaN  No   No  1.0  0.0   93.0   72.0   0.0  45.726  46.141  28.992   \n",
        "\n",
        "       C106  C107  C108  C109  C110  C111  C112  C113  C114  C115  C116  C117  \\\n",
        "0       NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
        "1  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   3.0   0.0   0.0   \n",
        "2  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   2.0   0.0   0.0   \n",
        "3  5.704305   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0   \n",
        "4  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   8.0   1.0   0.0   \n",
        "\n",
        "   C118  C119  C120  C121  C122         C123         C124  C125         C126  \\\n",
        "0   NaN   NaN   NaN   NaN   NaN          NaN          NaN   NaN          NaN   \n",
        "1   0.0   6.0   4.0   1.0   0.0  108701933.0  151396249.0   0.0  640602330.0   \n",
        "2   0.0   0.0   0.0   1.0   0.0          0.0          0.0   0.0          0.0   \n",
        "3   0.0   0.0   0.0   0.0   0.0          0.0          0.0   0.0          0.0   \n",
        "4   0.0   6.0  13.0   0.0   0.0          0.0     562953.0   0.0          0.0   \n",
        "\n",
        "   C127         C128        C129         C130  C131        C132          C133  \\\n",
        "0   NaN          NaN         NaN          NaN   NaN         NaN           NaN   \n",
        "1   0.0  113758047.0  85067978.0  135352031.0   0.0  51434728.0  1.795074e+09   \n",
        "2   0.0          0.0         0.0          0.0   0.0         0.0  0.000000e+00   \n",
        "3   0.0          0.0         0.0          0.0   0.0         0.0  0.000000e+00   \n",
        "4   0.0    6514284.0  45907574.0     581620.0   0.0         0.0  0.000000e+00   \n",
        "\n",
        "       C134         C135  C136  C137          C138  C139         C140  \\\n",
        "0       NaN          NaN   NaN   NaN           NaN   NaN          NaN   \n",
        "1  439151.0  326260868.0   0.0   0.0  1.906637e+09   0.0  557196876.0   \n",
        "2       0.0          0.0   0.0   0.0  0.000000e+00   0.0          0.0   \n",
        "3       0.0          0.0   0.0   0.0  0.000000e+00   0.0          0.0   \n",
        "4       0.0          0.0   0.0   0.0  1.128205e+06   0.0  567134600.0   \n",
        "\n",
        "           C141         C142  C143  C144          C145         C146   C147  \\\n",
        "0           NaN          NaN   NaN   NaN           NaN          NaN    NaN   \n",
        "1  5.169043e+09  448633971.0   0.0   0.0  1.716211e+09  570153410.0  99.35   \n",
        "2  0.000000e+00          0.0   0.0   0.0  0.000000e+00          0.0  25.48   \n",
        "3  0.000000e+00          0.0   0.0   0.0  0.000000e+00          0.0  22.35   \n",
        "4  2.739706e+08   21208770.0   0.0   0.0  0.000000e+00          0.0  58.42   \n",
        "\n",
        "    C148   C149   C150   C151  C152  C153   C154  C155  C156  C157  C158  \\\n",
        "0    NaN    NaN    NaN    NaN   NaN   NaN    NaN   NaN   NaN   NaN   NaN   \n",
        "1  77.32   7.21  12.93  12.93   0.0   0.0  79.21  59.0  24.0  19.0  13.0   \n",
        "2   6.35  19.14   0.00   0.00   0.0   0.0  13.52  16.0   4.0  12.0   0.0   \n",
        "3   6.14  13.64   2.57   2.57   0.0   0.0   6.14  16.0   8.0   6.0   2.0   \n",
        "4  14.80  16.85  26.27  26.27   0.0   0.0  26.27  51.0  12.0  10.0  28.0   \n",
        "\n",
        "   C159  C160  C161  C162  C163  C164  C165  C166  C167  C168  C169  C170  \\\n",
        "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
        "1  13.0   0.0   0.0  13.0   3.0   0.0   2.0   6.0   0.0   0.0   1.0   0.0   \n",
        "2   0.0   0.0   0.0   9.0   3.0   0.0   6.0   0.0   0.0   1.0   5.0   0.0   \n",
        "3   2.0   0.0   0.0   9.0   4.0   0.0   3.0   2.0   0.0   2.0   1.0   0.0   \n",
        "4  28.0   0.0   0.0  27.0   6.0   0.0   8.0  13.0   0.0   5.0   1.0   1.0   \n",
        "\n",
        "   C171  C172  C173  C174  C175  C176  C177  C178  C179  C180  C181  C182  \\\n",
        "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
        "1   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  13.0   \n",
        "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   9.0   \n",
        "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  10.0   \n",
        "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  27.0   \n",
        "\n",
        "   C183  C184  C185  C186   C187   C188  C189   C190  C191  C192  C193  C194  \\\n",
        "0   NaN   NaN   NaN   NaN   0.00   0.00   0.0   0.00  0.00   0.0  0.00   NaN   \n",
        "1   3.0   2.0   6.0   0.0  90.26  77.32   0.0  12.94  0.86   0.0  0.14  59.0   \n",
        "2   3.0   6.0   0.0   0.0   6.36   6.36   0.0   0.00  1.00   0.0  0.00  16.0   \n",
        "3   5.0   3.0   2.0   0.0   8.70   6.14   0.0   2.56  0.71   0.0  0.29  14.0   \n",
        "4   6.0   7.0  13.0   0.0  41.07  14.80   0.0  26.27  0.36   0.0  0.64  51.0   \n",
        "\n",
        "   C195  C196  C197  C198  C199  C200  C201  C202  C203  C204  C205  C206  \\\n",
        "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   0.0   0.0   0.0   \n",
        "1  24.0   0.0  19.0  13.0   0.0   0.0  17.0   0.0   2.0  37.0  24.0   0.0   \n",
        "2   4.0   0.0  12.0   0.0   0.0   3.0   9.0   0.0   0.0   4.0   4.0   0.0   \n",
        "3   6.0   0.0   6.0   2.0   0.0   3.0   3.0   0.0   0.0  10.0   8.0   0.0   \n",
        "4  12.0   0.0  11.0  28.0   0.0   6.0   1.0   3.0   0.0  40.0  12.0   0.0   \n",
        "\n",
        "   C207 C208  C209  C210   C211           C212      C213   C214          C215  \\\n",
        "0   0.0  NaN   NaN   0.0    0.0            NaN       NaN    NaN           NaN   \n",
        "1  13.0   \u696d\u8005   9.0   2.0  232.0  903739.278320  0.174970  232.0  5.289064e+09   \n",
        "2   0.0  \u7af6\u696dB   3.0   0.0    0.0            NaN       NaN    0.0  0.000000e+00   \n",
        "3   2.0   \u696d\u8005   7.0   1.0    0.0            NaN       NaN    0.0  0.000000e+00   \n",
        "4  28.0   \u56fa\u7db2  24.0   0.0  259.0   57456.744141  0.063761  259.0  9.227513e+08   \n",
        "\n",
        "   pred_C3  C3  \n",
        "0        0   0  \n",
        "1        0   0  \n",
        "2        1   1  \n",
        "3        1   0  \n",
        "4        1   0  "
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Prediction (each cluster)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Xp_cData = dfp.loc[:, choosed_for_clustering]\n",
      "\n",
      "for col in Xp_cData.columns:\n",
      "    Xp_cData[col].fillna(0, inplace=True)\n",
      "    Xp_cData[col] = StandardScaler().fit_transform(Xp_cData[col])\n",
      "\n",
      "dfp['clabel'] = kmeans.predict(Xp_cData)\n",
      "\n",
      "for i in xrange(3):\n",
      "    _X = dfp.loc[dfp['clabel']==i, choosed]\n",
      "    for col in _X.columns:\n",
      "      try:\n",
      "        _X[col].fillna(0, inplace=True)\n",
      "        _X[col] = StandardScaler().fit_transform(_X[col])\n",
      "      except:\n",
      "        _X[col] = _X[col].replace({name: n for n, name in enumerate(_X[col].unique())})\n",
      "    dfp.loc[dfp['clabel']==i, 'pred_C3'] = clfs[i].predict(_X)\n",
      "\n",
      "print accuracy_score(dfp['C3'], dfp['pred_C3'])\n",
      "\n",
      "dfp.tail()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.5899\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
        "  warnings.warn(msg, DataConversionWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:583: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
        "C:\\Python27\\lib\\site-packages\\sklearn\\preprocessing\\data.py:646: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
        "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>C2</th>\n",
        "      <th>C4</th>\n",
        "      <th>C5</th>\n",
        "      <th>C6</th>\n",
        "      <th>C7</th>\n",
        "      <th>C8</th>\n",
        "      <th>C9</th>\n",
        "      <th>C10</th>\n",
        "      <th>C11</th>\n",
        "      <th>C12</th>\n",
        "      <th>C13</th>\n",
        "      <th>C14</th>\n",
        "      <th>C15</th>\n",
        "      <th>C16</th>\n",
        "      <th>C17</th>\n",
        "      <th>C18</th>\n",
        "      <th>C19</th>\n",
        "      <th>C20</th>\n",
        "      <th>C21</th>\n",
        "      <th>C22</th>\n",
        "      <th>C23</th>\n",
        "      <th>C24</th>\n",
        "      <th>C25</th>\n",
        "      <th>C26</th>\n",
        "      <th>C27</th>\n",
        "      <th>C28</th>\n",
        "      <th>C29</th>\n",
        "      <th>C30</th>\n",
        "      <th>C31</th>\n",
        "      <th>C32</th>\n",
        "      <th>C33</th>\n",
        "      <th>C34</th>\n",
        "      <th>C35</th>\n",
        "      <th>C36</th>\n",
        "      <th>C37</th>\n",
        "      <th>C38</th>\n",
        "      <th>C39</th>\n",
        "      <th>C40</th>\n",
        "      <th>C41</th>\n",
        "      <th>C42</th>\n",
        "      <th>C43</th>\n",
        "      <th>C44</th>\n",
        "      <th>C45</th>\n",
        "      <th>C46</th>\n",
        "      <th>C47</th>\n",
        "      <th>C48</th>\n",
        "      <th>C49</th>\n",
        "      <th>C50</th>\n",
        "      <th>C51</th>\n",
        "      <th>C52</th>\n",
        "      <th>C53</th>\n",
        "      <th>C54</th>\n",
        "      <th>C55</th>\n",
        "      <th>C56</th>\n",
        "      <th>C57</th>\n",
        "      <th>C58</th>\n",
        "      <th>C59</th>\n",
        "      <th>C60</th>\n",
        "      <th>C61</th>\n",
        "      <th>C62</th>\n",
        "      <th>C63</th>\n",
        "      <th>C64</th>\n",
        "      <th>C65</th>\n",
        "      <th>C66</th>\n",
        "      <th>C67</th>\n",
        "      <th>C68</th>\n",
        "      <th>C69</th>\n",
        "      <th>C70</th>\n",
        "      <th>C71</th>\n",
        "      <th>C72</th>\n",
        "      <th>C73</th>\n",
        "      <th>C74</th>\n",
        "      <th>C75</th>\n",
        "      <th>C76</th>\n",
        "      <th>C77</th>\n",
        "      <th>C78</th>\n",
        "      <th>C79</th>\n",
        "      <th>C80</th>\n",
        "      <th>C81</th>\n",
        "      <th>C82</th>\n",
        "      <th>C83</th>\n",
        "      <th>C84</th>\n",
        "      <th>C85</th>\n",
        "      <th>C86</th>\n",
        "      <th>C87</th>\n",
        "      <th>C88</th>\n",
        "      <th>C89</th>\n",
        "      <th>C90</th>\n",
        "      <th>C91</th>\n",
        "      <th>C92</th>\n",
        "      <th>C93</th>\n",
        "      <th>C94</th>\n",
        "      <th>C95</th>\n",
        "      <th>C96</th>\n",
        "      <th>C97</th>\n",
        "      <th>C98</th>\n",
        "      <th>C99</th>\n",
        "      <th>C100</th>\n",
        "      <th>C101</th>\n",
        "      <th>C102</th>\n",
        "      <th>C103</th>\n",
        "      <th>C104</th>\n",
        "      <th>C105</th>\n",
        "      <th>C106</th>\n",
        "      <th>C107</th>\n",
        "      <th>C108</th>\n",
        "      <th>C109</th>\n",
        "      <th>C110</th>\n",
        "      <th>C111</th>\n",
        "      <th>C112</th>\n",
        "      <th>C113</th>\n",
        "      <th>C114</th>\n",
        "      <th>C115</th>\n",
        "      <th>C116</th>\n",
        "      <th>C117</th>\n",
        "      <th>C118</th>\n",
        "      <th>C119</th>\n",
        "      <th>C120</th>\n",
        "      <th>C121</th>\n",
        "      <th>C122</th>\n",
        "      <th>C123</th>\n",
        "      <th>C124</th>\n",
        "      <th>C125</th>\n",
        "      <th>C126</th>\n",
        "      <th>C127</th>\n",
        "      <th>C128</th>\n",
        "      <th>C129</th>\n",
        "      <th>C130</th>\n",
        "      <th>C131</th>\n",
        "      <th>C132</th>\n",
        "      <th>C133</th>\n",
        "      <th>C134</th>\n",
        "      <th>C135</th>\n",
        "      <th>C136</th>\n",
        "      <th>C137</th>\n",
        "      <th>C138</th>\n",
        "      <th>C139</th>\n",
        "      <th>C140</th>\n",
        "      <th>C141</th>\n",
        "      <th>C142</th>\n",
        "      <th>C143</th>\n",
        "      <th>C144</th>\n",
        "      <th>C145</th>\n",
        "      <th>C146</th>\n",
        "      <th>C147</th>\n",
        "      <th>C148</th>\n",
        "      <th>C149</th>\n",
        "      <th>C150</th>\n",
        "      <th>C151</th>\n",
        "      <th>C152</th>\n",
        "      <th>C153</th>\n",
        "      <th>C154</th>\n",
        "      <th>C155</th>\n",
        "      <th>C156</th>\n",
        "      <th>C157</th>\n",
        "      <th>C158</th>\n",
        "      <th>C159</th>\n",
        "      <th>C160</th>\n",
        "      <th>C161</th>\n",
        "      <th>C162</th>\n",
        "      <th>C163</th>\n",
        "      <th>C164</th>\n",
        "      <th>C165</th>\n",
        "      <th>C166</th>\n",
        "      <th>C167</th>\n",
        "      <th>C168</th>\n",
        "      <th>C169</th>\n",
        "      <th>C170</th>\n",
        "      <th>C171</th>\n",
        "      <th>C172</th>\n",
        "      <th>C173</th>\n",
        "      <th>C174</th>\n",
        "      <th>C175</th>\n",
        "      <th>C176</th>\n",
        "      <th>C177</th>\n",
        "      <th>C178</th>\n",
        "      <th>C179</th>\n",
        "      <th>C180</th>\n",
        "      <th>C181</th>\n",
        "      <th>C182</th>\n",
        "      <th>C183</th>\n",
        "      <th>C184</th>\n",
        "      <th>C185</th>\n",
        "      <th>C186</th>\n",
        "      <th>C187</th>\n",
        "      <th>C188</th>\n",
        "      <th>C189</th>\n",
        "      <th>C190</th>\n",
        "      <th>C191</th>\n",
        "      <th>C192</th>\n",
        "      <th>C193</th>\n",
        "      <th>C194</th>\n",
        "      <th>C195</th>\n",
        "      <th>C196</th>\n",
        "      <th>C197</th>\n",
        "      <th>C198</th>\n",
        "      <th>C199</th>\n",
        "      <th>C200</th>\n",
        "      <th>C201</th>\n",
        "      <th>C202</th>\n",
        "      <th>C203</th>\n",
        "      <th>C204</th>\n",
        "      <th>C205</th>\n",
        "      <th>C206</th>\n",
        "      <th>C207</th>\n",
        "      <th>C208</th>\n",
        "      <th>C209</th>\n",
        "      <th>C210</th>\n",
        "      <th>C211</th>\n",
        "      <th>C212</th>\n",
        "      <th>C213</th>\n",
        "      <th>C214</th>\n",
        "      <th>C215</th>\n",
        "      <th>pred_C3</th>\n",
        "      <th>C3</th>\n",
        "      <th>clabel</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>9995</th>\n",
        "      <td>C0294226</td>\n",
        "      <td>NVIP</td>\n",
        "      <td>F</td>\n",
        "      <td>47.0</td>\n",
        "      <td>\u5929\u79e4\u5ea7</td>\n",
        "      <td>88</td>\n",
        "      <td>\u64da\u9ede05</td>\n",
        "      <td>Y</td>\n",
        "      <td>\u4e2d</td>\n",
        "      <td>4G</td>\n",
        "      <td>1336.0</td>\n",
        "      <td>\u901a\u4fe1\u7d04</td>\n",
        "      <td>NaN</td>\n",
        "      <td>4</td>\n",
        "      <td>4</td>\n",
        "      <td>\u4e2d</td>\n",
        "      <td>\u4f4e\u4e2d</td>\n",
        "      <td>2.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>\u7121</td>\n",
        "      <td>N</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>\u7af6\u696dB</td>\n",
        "      <td>86.0</td>\n",
        "      <td>86.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>3</td>\n",
        "      <td>3</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>3</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>1.567282e+06</td>\n",
        "      <td>108.99</td>\n",
        "      <td>33.08</td>\n",
        "      <td>0.0</td>\n",
        "      <td>62.34</td>\n",
        "      <td>13.32</td>\n",
        "      <td>0.0</td>\n",
        "      <td>25.57</td>\n",
        "      <td>34.97</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.8</td>\n",
        "      <td>0.30</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.57</td>\n",
        "      <td>0.12</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.23</td>\n",
        "      <td>0.32</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.02</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.25</td>\n",
        "      <td>2.77</td>\n",
        "      <td>7.70</td>\n",
        "      <td>33.17</td>\n",
        "      <td>7.47</td>\n",
        "      <td>12.62</td>\n",
        "      <td>10.92</td>\n",
        "      <td>31.07</td>\n",
        "      <td>1.77</td>\n",
        "      <td>1.27</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.03</td>\n",
        "      <td>0.07</td>\n",
        "      <td>0.30</td>\n",
        "      <td>0.07</td>\n",
        "      <td>0.12</td>\n",
        "      <td>0.10</td>\n",
        "      <td>0.29</td>\n",
        "      <td>0.02</td>\n",
        "      <td>0.01</td>\n",
        "      <td>\u5c4f\u6771\u7e23</td>\n",
        "      <td>\u5c4f\u6771\u5e02</td>\n",
        "      <td>\u5c4f\u6771\u7e23</td>\n",
        "      <td>\u5c4f\u6771\u5e02</td>\n",
        "      <td>No</td>\n",
        "      <td>No</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>228.0</td>\n",
        "      <td>15.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>307.476</td>\n",
        "      <td>41.996</td>\n",
        "      <td>253.324</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>10.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>12.0</td>\n",
        "      <td>12.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>16.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>26.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>14259319.0</td>\n",
        "      <td>439665398.0</td>\n",
        "      <td>2.949158e+08</td>\n",
        "      <td>7.857726e+07</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>42562618.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>11569936.0</td>\n",
        "      <td>305027092.0</td>\n",
        "      <td>7.929694e+08</td>\n",
        "      <td>7.430331e+08</td>\n",
        "      <td>4.815118e+08</td>\n",
        "      <td>0.0</td>\n",
        "      <td>9922937.0</td>\n",
        "      <td>76581435.0</td>\n",
        "      <td>9431279.0</td>\n",
        "      <td>108.98</td>\n",
        "      <td>33.09</td>\n",
        "      <td>62.34</td>\n",
        "      <td>13.32</td>\n",
        "      <td>13.32</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>33.34</td>\n",
        "      <td>103.0</td>\n",
        "      <td>57.0</td>\n",
        "      <td>37.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>29.0</td>\n",
        "      <td>10.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>13.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>9.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>29.0</td>\n",
        "      <td>10.0</td>\n",
        "      <td>13.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>46.41</td>\n",
        "      <td>33.08</td>\n",
        "      <td>0.0</td>\n",
        "      <td>13.33</td>\n",
        "      <td>0.71</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.29</td>\n",
        "      <td>103.0</td>\n",
        "      <td>57.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>37.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>28.0</td>\n",
        "      <td>7.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>65.0</td>\n",
        "      <td>57.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>\u696d\u8005</td>\n",
        "      <td>26.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>195.0</td>\n",
        "      <td>1.928009e+05</td>\n",
        "      <td>0.123016</td>\n",
        "      <td>195.0</td>\n",
        "      <td>1.604897e+09</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>2</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9996</th>\n",
        "      <td>C0263886</td>\n",
        "      <td>NVIP</td>\n",
        "      <td>F</td>\n",
        "      <td>59.0</td>\n",
        "      <td>\u7261\u7f8a\u5ea7</td>\n",
        "      <td>188</td>\n",
        "      <td>\u64da\u9ede07</td>\n",
        "      <td>N</td>\n",
        "      <td>\u6975\u4f4e</td>\n",
        "      <td>3G</td>\n",
        "      <td>183.0</td>\n",
        "      <td>\u8cfc\u6a5f\u7d04</td>\n",
        "      <td>-2.0</td>\n",
        "      <td>3</td>\n",
        "      <td>2</td>\n",
        "      <td>\u4f4e\u4e2d</td>\n",
        "      <td>\u4f4e\u4e2d</td>\n",
        "      <td>NaN</td>\n",
        "      <td>1.0</td>\n",
        "      <td>\u7af6\u696dB</td>\n",
        "      <td>N</td>\n",
        "      <td>1</td>\n",
        "      <td>NaN</td>\n",
        "      <td>1.0</td>\n",
        "      <td>NaN</td>\n",
        "      <td>1</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>\u7121NP</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>2</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>2</td>\n",
        "      <td>NaN</td>\n",
        "      <td>Yes</td>\n",
        "      <td>No</td>\n",
        "      <td>NaN</td>\n",
        "      <td>16.45</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>16.45</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>1.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>5.67</td>\n",
        "      <td>10.38</td>\n",
        "      <td>0.40</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.34</td>\n",
        "      <td>0.63</td>\n",
        "      <td>0.02</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>16.45</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>16.45</td>\n",
        "      <td>16.45</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>16.45</td>\n",
        "      <td>17.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>17.0</td>\n",
        "      <td>17.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>16.45</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>16.45</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.00</td>\n",
        "      <td>17.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>17.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>17.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>17.0</td>\n",
        "      <td>\u56fa\u7db2</td>\n",
        "      <td>3.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9997</th>\n",
        "      <td>C0167683</td>\n",
        "      <td>NVIP</td>\n",
        "      <td>M</td>\n",
        "      <td>32.0</td>\n",
        "      <td>\u5929\u880d\u5ea7</td>\n",
        "      <td>129</td>\n",
        "      <td>\u64da\u9ede06</td>\n",
        "      <td>N</td>\n",
        "      <td>\u4e2d</td>\n",
        "      <td>4G</td>\n",
        "      <td>1336.0</td>\n",
        "      <td>\u901a\u4fe1\u7d04</td>\n",
        "      <td>-31.0</td>\n",
        "      <td>3</td>\n",
        "      <td>3</td>\n",
        "      <td>\u4e2d</td>\n",
        "      <td>\u4e2d</td>\n",
        "      <td>1.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>\u7af6\u696dC</td>\n",
        "      <td>N</td>\n",
        "      <td>1</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>1</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>\u7121NP</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>Yes</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>2.453259e+07</td>\n",
        "      <td>144.78</td>\n",
        "      <td>115.93</td>\n",
        "      <td>0.0</td>\n",
        "      <td>18.77</td>\n",
        "      <td>10.08</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>18.77</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.80</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.13</td>\n",
        "      <td>0.07</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.13</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>5.3</td>\n",
        "      <td>0.72</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.28</td>\n",
        "      <td>11.20</td>\n",
        "      <td>8.67</td>\n",
        "      <td>16.10</td>\n",
        "      <td>5.20</td>\n",
        "      <td>55.65</td>\n",
        "      <td>30.87</td>\n",
        "      <td>10.17</td>\n",
        "      <td>0.63</td>\n",
        "      <td>0.04</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.08</td>\n",
        "      <td>0.06</td>\n",
        "      <td>0.11</td>\n",
        "      <td>0.04</td>\n",
        "      <td>0.38</td>\n",
        "      <td>0.21</td>\n",
        "      <td>0.07</td>\n",
        "      <td>0.00</td>\n",
        "      <td>\u65b0\u5317\u5e02</td>\n",
        "      <td>\u571f\u57ce\u5340</td>\n",
        "      <td>\u65b0\u5317\u5e02</td>\n",
        "      <td>\u571f\u57ce\u5340</td>\n",
        "      <td>No</td>\n",
        "      <td>No</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>51.077</td>\n",
        "      <td>115.313</td>\n",
        "      <td>13.258</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>6.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>13.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>9.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>14.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>17.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>612945661.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>177152309.0</td>\n",
        "      <td>60247083.0</td>\n",
        "      <td>532005101.0</td>\n",
        "      <td>1.634267e+09</td>\n",
        "      <td>1.884671e+09</td>\n",
        "      <td>0.0</td>\n",
        "      <td>152925231.0</td>\n",
        "      <td>21941803.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>3.000222e+09</td>\n",
        "      <td>52418707.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>231547895.0</td>\n",
        "      <td>3.251698e+09</td>\n",
        "      <td>1.069934e+10</td>\n",
        "      <td>5.767356e+09</td>\n",
        "      <td>0.0</td>\n",
        "      <td>91557009.0</td>\n",
        "      <td>679913317.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>144.78</td>\n",
        "      <td>115.93</td>\n",
        "      <td>18.77</td>\n",
        "      <td>10.08</td>\n",
        "      <td>10.08</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>115.93</td>\n",
        "      <td>73.0</td>\n",
        "      <td>57.0</td>\n",
        "      <td>11.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>18.0</td>\n",
        "      <td>10.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>18.0</td>\n",
        "      <td>10.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>126.00</td>\n",
        "      <td>115.92</td>\n",
        "      <td>0.0</td>\n",
        "      <td>10.08</td>\n",
        "      <td>0.92</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.08</td>\n",
        "      <td>73.0</td>\n",
        "      <td>57.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>11.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>11.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>62.0</td>\n",
        "      <td>57.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>\u696d\u8005</td>\n",
        "      <td>26.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>227.0</td>\n",
        "      <td>1.653876e+06</td>\n",
        "      <td>0.067415</td>\n",
        "      <td>227.0</td>\n",
        "      <td>2.512137e+10</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9998</th>\n",
        "      <td>C0132373</td>\n",
        "      <td>VIP4</td>\n",
        "      <td>M</td>\n",
        "      <td>62.0</td>\n",
        "      <td>\u5929\u79e4\u5ea7</td>\n",
        "      <td>248</td>\n",
        "      <td>\u64da\u9ede13</td>\n",
        "      <td>N</td>\n",
        "      <td>\u4e2d</td>\n",
        "      <td>3G</td>\n",
        "      <td>983.0</td>\n",
        "      <td>\u8cfc\u6a5f\u7d04</td>\n",
        "      <td>-50.0</td>\n",
        "      <td>5</td>\n",
        "      <td>5</td>\n",
        "      <td>\u4f4e\u4e2d</td>\n",
        "      <td>\u96f6\u5143</td>\n",
        "      <td>NaN</td>\n",
        "      <td>3.0</td>\n",
        "      <td>\u7121</td>\n",
        "      <td>N</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>\u7121NP</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>4</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>1</td>\n",
        "      <td>0</td>\n",
        "      <td>3</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>NaN</td>\n",
        "      <td>257.13</td>\n",
        "      <td>46.80</td>\n",
        "      <td>0.0</td>\n",
        "      <td>149.55</td>\n",
        "      <td>60.58</td>\n",
        "      <td>0.0</td>\n",
        "      <td>115.73</td>\n",
        "      <td>33.82</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.18</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.58</td>\n",
        "      <td>0.24</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.45</td>\n",
        "      <td>0.13</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.70</td>\n",
        "      <td>4.35</td>\n",
        "      <td>47.37</td>\n",
        "      <td>122.25</td>\n",
        "      <td>24.65</td>\n",
        "      <td>30.93</td>\n",
        "      <td>4.32</td>\n",
        "      <td>10.50</td>\n",
        "      <td>6.12</td>\n",
        "      <td>5.95</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.02</td>\n",
        "      <td>0.18</td>\n",
        "      <td>0.48</td>\n",
        "      <td>0.10</td>\n",
        "      <td>0.12</td>\n",
        "      <td>0.02</td>\n",
        "      <td>0.04</td>\n",
        "      <td>0.02</td>\n",
        "      <td>0.02</td>\n",
        "      <td>\u82d7\u6817\u7e23</td>\n",
        "      <td>\u901a\u9704\u93ae</td>\n",
        "      <td>\\N</td>\n",
        "      <td>\\N</td>\n",
        "      <td>No</td>\n",
        "      <td>No</td>\n",
        "      <td>12.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>163.0</td>\n",
        "      <td>75.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>480.431</td>\n",
        "      <td>131.628</td>\n",
        "      <td>129.690</td>\n",
        "      <td>0.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>16.0</td>\n",
        "      <td>15.0</td>\n",
        "      <td>9.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>12.0</td>\n",
        "      <td>14.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>10.0</td>\n",
        "      <td>17.0</td>\n",
        "      <td>24.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>257.13</td>\n",
        "      <td>46.80</td>\n",
        "      <td>149.55</td>\n",
        "      <td>60.58</td>\n",
        "      <td>60.58</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>115.73</td>\n",
        "      <td>136.0</td>\n",
        "      <td>32.0</td>\n",
        "      <td>68.0</td>\n",
        "      <td>35.0</td>\n",
        "      <td>35.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>23.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>9.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>6.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>23.0</td>\n",
        "      <td>8.0</td>\n",
        "      <td>9.0</td>\n",
        "      <td>5.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>107.38</td>\n",
        "      <td>46.79</td>\n",
        "      <td>0.0</td>\n",
        "      <td>60.59</td>\n",
        "      <td>0.44</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.56</td>\n",
        "      <td>136.0</td>\n",
        "      <td>32.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>68.0</td>\n",
        "      <td>35.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>37.0</td>\n",
        "      <td>31.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>67.0</td>\n",
        "      <td>32.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>35.0</td>\n",
        "      <td>\u7af6\u696dA</td>\n",
        "      <td>54.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9999</th>\n",
        "      <td>C0010487</td>\n",
        "      <td>NVIP</td>\n",
        "      <td>M</td>\n",
        "      <td>45.0</td>\n",
        "      <td>\u5929\u880d\u5ea7</td>\n",
        "      <td>74</td>\n",
        "      <td>\u64da\u9ede02</td>\n",
        "      <td>N</td>\n",
        "      <td>\u6975\u4f4e</td>\n",
        "      <td>3G</td>\n",
        "      <td>183.0</td>\n",
        "      <td>\u7121\u7d81\u7d04</td>\n",
        "      <td>NaN</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>\u96f6\u5143</td>\n",
        "      <td>\u96f6\u5143</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>\u7af6\u696dD</td>\n",
        "      <td>N</td>\n",
        "      <td>1</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>1</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>\u7121NP</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>4</td>\n",
        "      <td>4</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>0</td>\n",
        "      <td>3</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>NaN</td>\n",
        "      <td>6.86</td>\n",
        "      <td>4.43</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>2.43</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.65</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.35</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>1.27</td>\n",
        "      <td>1.50</td>\n",
        "      <td>0.13</td>\n",
        "      <td>0.00</td>\n",
        "      <td>2.97</td>\n",
        "      <td>0.23</td>\n",
        "      <td>0.77</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.18</td>\n",
        "      <td>0.22</td>\n",
        "      <td>0.02</td>\n",
        "      <td>0.00</td>\n",
        "      <td>0.43</td>\n",
        "      <td>0.03</td>\n",
        "      <td>0.11</td>\n",
        "      <td>0.00</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>No</td>\n",
        "      <td>No</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>6.87</td>\n",
        "      <td>4.43</td>\n",
        "      <td>0.00</td>\n",
        "      <td>2.43</td>\n",
        "      <td>2.43</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>4.43</td>\n",
        "      <td>15.0</td>\n",
        "      <td>13.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>4.0</td>\n",
        "      <td>3.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>1.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>6.86</td>\n",
        "      <td>4.43</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2.43</td>\n",
        "      <td>0.65</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.35</td>\n",
        "      <td>15.0</td>\n",
        "      <td>13.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>15.0</td>\n",
        "      <td>13.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>2.0</td>\n",
        "      <td>\u696d\u8005</td>\n",
        "      <td>6.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.0</td>\n",
        "      <td>NaN</td>\n",
        "      <td>NaN</td>\n",
        "      <td>0.0</td>\n",
        "      <td>0.000000e+00</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>1</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "            C2    C4 C5    C6   C7   C8    C9 C10 C11 C12     C13  C14   C15  \\\n",
        "9995  C0294226  NVIP  F  47.0  \u5929\u79e4\u5ea7   88  \u64da\u9ede05   Y   \u4e2d  4G  1336.0  \u901a\u4fe1\u7d04   NaN   \n",
        "9996  C0263886  NVIP  F  59.0  \u7261\u7f8a\u5ea7  188  \u64da\u9ede07   N  \u6975\u4f4e  3G   183.0  \u8cfc\u6a5f\u7d04  -2.0   \n",
        "9997  C0167683  NVIP  M  32.0  \u5929\u880d\u5ea7  129  \u64da\u9ede06   N   \u4e2d  4G  1336.0  \u901a\u4fe1\u7d04 -31.0   \n",
        "9998  C0132373  VIP4  M  62.0  \u5929\u79e4\u5ea7  248  \u64da\u9ede13   N   \u4e2d  3G   983.0  \u8cfc\u6a5f\u7d04 -50.0   \n",
        "9999  C0010487  NVIP  M  45.0  \u5929\u880d\u5ea7   74  \u64da\u9ede02   N  \u6975\u4f4e  3G   183.0  \u7121\u7d81\u7d04   NaN   \n",
        "\n",
        "      C16  C17 C18 C19  C20  C21  C22 C23  C24  C25  C26  C27  C28  C29  C30  \\\n",
        "9995    4    4   \u4e2d  \u4f4e\u4e2d  2.0  4.0    \u7121   N    0  0.0  0.0  0.0    0  0.0  0.0   \n",
        "9996    3    2  \u4f4e\u4e2d  \u4f4e\u4e2d  NaN  1.0  \u7af6\u696dB   N    1  NaN  1.0  NaN    1  NaN  NaN   \n",
        "9997    3    3   \u4e2d   \u4e2d  1.0  2.0  \u7af6\u696dC   N    1  NaN  NaN  NaN    1  NaN  NaN   \n",
        "9998    5    5  \u4f4e\u4e2d  \u96f6\u5143  NaN  3.0    \u7121   N    0  0.0  0.0  0.0    0  0.0  0.0   \n",
        "9999    0    0  \u96f6\u5143  \u96f6\u5143  NaN  NaN  \u7af6\u696dD   N    1  NaN  NaN  NaN    1  NaN  NaN   \n",
        "\n",
        "      C31   C32   C33  C34  C35  C36  C37  C38  C39  C40  C41  C42  C43  C44  \\\n",
        "9995  \u7af6\u696dB  86.0  86.0  1.0    3    3    0    0    0    0    0    0    3  NaN   \n",
        "9996  \u7121NP   NaN   NaN  NaN    2    1    0    1    0    0    0    0    2  NaN   \n",
        "9997  \u7121NP   NaN   NaN  NaN    1    1    0    0    0    0    0    0    1  Yes   \n",
        "9998  \u7121NP   NaN   NaN  NaN    4    1    1    1    0    0    1    0    3  NaN   \n",
        "9999  \u7121NP   NaN   NaN  NaN    4    4    0    0    0    0    0    0    3  NaN   \n",
        "\n",
        "      C45 C46           C47     C48     C49  C50     C51    C52  C53     C54  \\\n",
        "9995  NaN  No  1.567282e+06  108.99   33.08  0.0   62.34  13.32  0.0   25.57   \n",
        "9996  Yes  No           NaN   16.45    0.00  0.0    0.00  16.45  0.0    0.00   \n",
        "9997  NaN  No  2.453259e+07  144.78  115.93  0.0   18.77  10.08  0.0    0.00   \n",
        "9998  NaN  No           NaN  257.13   46.80  0.0  149.55  60.58  0.0  115.73   \n",
        "9999  NaN  No           NaN    6.86    4.43  0.0    0.00   2.43  0.0    0.00   \n",
        "\n",
        "        C55  C56  C57   C58  C59   C60   C61  C62  C63   C64   C65  C66   C67  \\\n",
        "9995  34.97  0.0  1.8  0.30  0.0  0.57  0.12  0.0  0.0  0.23  0.32  0.0  0.02   \n",
        "9996   0.00  0.0  0.0  0.00  0.0  0.00  1.00  0.0  0.0  0.00  0.00  0.0  0.00   \n",
        "9997  18.77  0.0  0.0  0.80  0.0  0.13  0.07  0.0  0.0  0.00  0.13  0.0  0.00   \n",
        "9998  33.82  0.0  0.0  0.18  0.0  0.58  0.24  0.0  0.0  0.45  0.13  0.0  0.00   \n",
        "9999   0.00  0.0  0.0  0.65  0.0  0.00  0.35  0.0  0.0  0.00  0.00  0.0  0.00   \n",
        "\n",
        "      C68   C69   C70   C71    C72     C73    C74    C75    C76    C77    C78  \\\n",
        "9995  0.0  0.00  0.25  2.77   7.70   33.17   7.47  12.62  10.92  31.07   1.77   \n",
        "9996  0.0  0.00  0.00  0.00   0.00    5.67  10.38   0.40   0.00   0.00   0.00   \n",
        "9997  5.3  0.72  0.00  0.28  11.20    8.67  16.10   5.20  55.65  30.87  10.17   \n",
        "9998  0.0  0.00  0.70  4.35  47.37  122.25  24.65  30.93   4.32  10.50   6.12   \n",
        "9999  0.0  0.00  0.00  0.00   1.27    1.50   0.13   0.00   2.97   0.23   0.77   \n",
        "\n",
        "       C79   C80  C81  C82   C83   C84   C85   C86   C87   C88   C89   C90  \\\n",
        "9995  1.27  0.00  0.0  0.0  0.03  0.07  0.30  0.07  0.12  0.10  0.29  0.02   \n",
        "9996  0.00  0.00  0.0  0.0  0.00  0.00  0.34  0.63  0.02  0.00  0.00  0.00   \n",
        "9997  0.63  0.04  0.0  0.0  0.00  0.08  0.06  0.11  0.04  0.38  0.21  0.07   \n",
        "9998  5.95  0.00  0.0  0.0  0.02  0.18  0.48  0.10  0.12  0.02  0.04  0.02   \n",
        "9999  0.00  0.00  0.0  0.0  0.00  0.18  0.22  0.02  0.00  0.43  0.03  0.11   \n",
        "\n",
        "       C91  C92  C93  C94  C95  C96  C97   C98  C99   C100  C101  C102  \\\n",
        "9995  0.01  \u5c4f\u6771\u7e23  \u5c4f\u6771\u5e02  \u5c4f\u6771\u7e23  \u5c4f\u6771\u5e02   No   No   0.0  1.0  228.0  15.0   0.0   \n",
        "9996  0.00  NaN  NaN  NaN  NaN  NaN  NaN   NaN  NaN    NaN   NaN   NaN   \n",
        "9997  0.00  \u65b0\u5317\u5e02  \u571f\u57ce\u5340  \u65b0\u5317\u5e02  \u571f\u57ce\u5340   No   No   0.0  0.0    0.0   0.0   0.0   \n",
        "9998  0.02  \u82d7\u6817\u7e23  \u901a\u9704\u93ae   \\N   \\N   No   No  12.0  4.0  163.0  75.0   0.0   \n",
        "9999  0.00  NaN  NaN  NaN  NaN   No   No   NaN  NaN    NaN   NaN   NaN   \n",
        "\n",
        "         C103     C104     C105  C106  C107  C108  C109  C110  C111  C112  \\\n",
        "9995  307.476   41.996  253.324   0.0   0.0   0.0   0.0   0.0  10.0   0.0   \n",
        "9996      NaN      NaN      NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
        "9997   51.077  115.313   13.258   0.0   2.0   0.0   0.0   0.0   6.0   3.0   \n",
        "9998  480.431  131.628  129.690   0.0   3.0   0.0   0.0   0.0  16.0  15.0   \n",
        "9999      NaN      NaN      NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
        "\n",
        "      C113  C114  C115  C116  C117  C118  C119  C120  C121  C122  C123  \\\n",
        "9995   4.0   0.0  12.0  12.0   1.0  16.0   4.0  26.0   1.0   1.0   0.0   \n",
        "9996   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
        "9997  13.0   1.0   9.0   2.0   0.0  14.0   5.0  17.0   1.0   0.0   0.0   \n",
        "9998   9.0   2.0  12.0  14.0   0.0  10.0  17.0  24.0   5.0   0.0   0.0   \n",
        "9999   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
        "\n",
        "             C124  C125         C126        C127         C128          C129  \\\n",
        "9995          0.0   0.0          0.0  14259319.0  439665398.0  2.949158e+08   \n",
        "9996          NaN   NaN          NaN         NaN          NaN           NaN   \n",
        "9997  612945661.0   0.0  177152309.0  60247083.0  532005101.0  1.634267e+09   \n",
        "9998          0.0   0.0          0.0         0.0          0.0  0.000000e+00   \n",
        "9999          NaN   NaN          NaN         NaN          NaN           NaN   \n",
        "\n",
        "              C130  C131         C132        C133  C134          C135  \\\n",
        "9995  7.857726e+07   0.0          0.0         0.0   0.0  0.000000e+00   \n",
        "9996           NaN   NaN          NaN         NaN   NaN           NaN   \n",
        "9997  1.884671e+09   0.0  152925231.0  21941803.0   0.0  3.000222e+09   \n",
        "9998  0.000000e+00   0.0          0.0         0.0   0.0  0.000000e+00   \n",
        "9999           NaN   NaN          NaN         NaN   NaN           NaN   \n",
        "\n",
        "            C136  C137        C138         C139          C140          C141  \\\n",
        "9995  42562618.0   0.0  11569936.0  305027092.0  7.929694e+08  7.430331e+08   \n",
        "9996         NaN   NaN         NaN          NaN           NaN           NaN   \n",
        "9997  52418707.0   0.0         0.0  231547895.0  3.251698e+09  1.069934e+10   \n",
        "9998         0.0   0.0         0.0          0.0  0.000000e+00  0.000000e+00   \n",
        "9999         NaN   NaN         NaN          NaN           NaN           NaN   \n",
        "\n",
        "              C142  C143        C144         C145       C146    C147    C148  \\\n",
        "9995  4.815118e+08   0.0   9922937.0   76581435.0  9431279.0  108.98   33.09   \n",
        "9996           NaN   NaN         NaN          NaN        NaN   16.45    0.00   \n",
        "9997  5.767356e+09   0.0  91557009.0  679913317.0        0.0  144.78  115.93   \n",
        "9998  0.000000e+00   0.0         0.0          0.0        0.0  257.13   46.80   \n",
        "9999           NaN   NaN         NaN          NaN        NaN    6.87    4.43   \n",
        "\n",
        "        C149   C150   C151  C152  C153    C154   C155  C156  C157  C158  C159  \\\n",
        "9995   62.34  13.32  13.32   0.0   0.0   33.34  103.0  57.0  37.0   8.0   8.0   \n",
        "9996    0.00  16.45  16.45   0.0   0.0   16.45   17.0   0.0   0.0  17.0  17.0   \n",
        "9997   18.77  10.08  10.08   0.0   0.0  115.93   73.0  57.0  11.0   5.0   5.0   \n",
        "9998  149.55  60.58  60.58   0.0   0.0  115.73  136.0  32.0  68.0  35.0  35.0   \n",
        "9999    0.00   2.43   2.43   0.0   0.0    4.43   15.0  13.0   0.0   2.0   2.0   \n",
        "\n",
        "      C160  C161  C162  C163  C164  C165  C166  C167  C168  C169  C170  C171  \\\n",
        "9995   0.0   0.0  29.0  10.0   0.0  13.0   5.0   0.0   9.0   3.0   0.0   1.0   \n",
        "9996   0.0   0.0   2.0   0.0   0.0   0.0   2.0   0.0   0.0   0.0   0.0   0.0   \n",
        "9997   0.0   0.0  18.0  10.0   0.0   5.0   3.0   0.0   0.0   5.0   0.0   0.0   \n",
        "9998   0.0   0.0  23.0   8.0   0.0   9.0   5.0   0.0   3.0   6.0   0.0   0.0   \n",
        "9999   0.0   0.0   4.0   3.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
        "\n",
        "      C172  C173  C174  C175  C176  C177  C178  C179  C180  C181  C182  C183  \\\n",
        "9995   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  29.0  10.0   \n",
        "9996   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   2.0   0.0   \n",
        "9997   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  18.0  10.0   \n",
        "9998   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  23.0   8.0   \n",
        "9999   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   4.0   3.0   \n",
        "\n",
        "      C184  C185  C186    C187    C188  C189   C190  C191  C192  C193   C194  \\\n",
        "9995  13.0   5.0   0.0   46.41   33.08   0.0  13.33  0.71   0.0  0.29  103.0   \n",
        "9996   0.0   2.0   0.0   16.45    0.00   0.0  16.45  0.00   0.0  1.00   17.0   \n",
        "9997   5.0   3.0   0.0  126.00  115.92   0.0  10.08  0.92   0.0  0.08   73.0   \n",
        "9998   9.0   5.0   0.0  107.38   46.79   0.0  60.59  0.44   0.0  0.56  136.0   \n",
        "9999   0.0   1.0   0.0    6.86    4.43   0.0   2.43  0.65   0.0  0.35   15.0   \n",
        "\n",
        "      C195  C196  C197  C198  C199  C200  C201  C202  C203  C204  C205  C206  \\\n",
        "9995  57.0   0.0  37.0   8.0   0.0  28.0   7.0   0.0   2.0  65.0  57.0   0.0   \n",
        "9996   0.0   0.0   0.0  17.0   0.0   0.0   0.0   0.0   0.0  17.0   0.0   0.0   \n",
        "9997  57.0   0.0  11.0   5.0   0.0   0.0  11.0   0.0   0.0  62.0  57.0   0.0   \n",
        "9998  32.0   0.0  68.0  35.0   0.0  37.0  31.0   0.0   0.0  67.0  32.0   0.0   \n",
        "9999  13.0   0.0   0.0   2.0   0.0   0.0   0.0   0.0   0.0  15.0  13.0   0.0   \n",
        "\n",
        "      C207 C208  C209  C210   C211          C212      C213   C214  \\\n",
        "9995   8.0   \u696d\u8005  26.0   0.0  195.0  1.928009e+05  0.123016  195.0   \n",
        "9996  17.0   \u56fa\u7db2   3.0   0.0    0.0           NaN       NaN    0.0   \n",
        "9997   5.0   \u696d\u8005  26.0   0.0  227.0  1.653876e+06  0.067415  227.0   \n",
        "9998  35.0  \u7af6\u696dA  54.0   0.0    0.0           NaN       NaN    0.0   \n",
        "9999   2.0   \u696d\u8005   6.0   0.0    0.0           NaN       NaN    0.0   \n",
        "\n",
        "              C215  pred_C3  C3  clabel  \n",
        "9995  1.604897e+09        0   0       2  \n",
        "9996  0.000000e+00        1   1       0  \n",
        "9997  2.512137e+10        1   1       1  \n",
        "9998  0.000000e+00        0   0       0  \n",
        "9999  0.000000e+00        1   1       1  "
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Ploting"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import roc_curve\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fpr1, tpr1, _ = roc_curve(y_test, clf1.predict_proba(X_test)[:, 1])\n",
      "fpr2, tpr2, _ = roc_curve(y_test, clf2.predict_proba(X_test)[:, 1])\n",
      "fpr3, tpr3, _ = roc_curve(y_test, clf3.predict_proba(X_test)[:, 1])\n",
      "fpr4, tpr4, _ = roc_curve(y_test, clf4.predict_proba(X_test)[:, 1])\n",
      "fprv, tprv, _ = roc_curve(y_test, vclf.predict_proba(X_test)[:, 1])\n",
      "\n",
      "plt.plot(fpr1, tpr1, label='Naive Bayes')\n",
      "plt.plot(fpr2, tpr2, label='Logistic Regression')\n",
      "plt.plot(fpr3, tpr3, label='Random Forest')\n",
      "plt.plot(fpr4, tpr4, label='SVM')\n",
      "plt.plot(fprv, tprv, label='Voting Classifier')\n",
      "\n",
      "plt.plot([0, 1], [0, 1], 'k--')\n",
      "plt.xlim([0.0, 1.0])\n",
      "plt.ylim([0.0, 1.05])\n",
      "plt.xlabel('False Positive Rate')\n",
      "plt.ylabel('True Positive Rate')\n",
      "plt.title('ROC curve')\n",
      "plt.legend(loc=\"lower right\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGAwGQwKmGBgMBoMhwf8DjEgcflsHQP0AAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x9572650>"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "C001 \u6708\u4efd C002 \u8a2d\u5099\u6d41\u6c34\u865f C003 \u662f\u5426NPOUT C004 VIP\u7b49\u7d1a C005 \u6027\u5225[\u7528\u6236] C006 \u5e74\u9f61[\u7528\u6236] C007 \u661f\u5ea7[\u7528\u6236] C008 [\u884c\u52d5]\u5e74\u8cc7 C009 \u64da\u9ede C010 \u662f\u5426\u70ba\u7279\u7d04\n",
      "\n",
      "C011 [\u7528\u6236]\u5e33\u55ae\u7e3d\u91d1\u984d(\u4f4e/\u4f4e\u4e2d/\u4e2d/\u4e2d\u9ad8/\u9ad8) C012 [\u884c\u52d5]\u9580\u865f\u8cc7\u8cbb\u985e\u578b C013 [\u884c\u52d5]\u9580\u865f\u8cc7\u8cbb\u8cbb\u7387 C014 \u6700\u5f8c\u7d81\u7d04\u985e\u578b C015 \u7d81\u7d04\u5230\u671f\u6708\u6578 C016 \u624b\u6a5f\u7d81\u7d04\u6b21\u6578(\u81ea\u958b\u53f0) C017 \u624b\u6a5f\u8cfc\u8cb7\u6b21\u6578(\u81ea\u958b\u53f0) C018 \u624b\u6a5f\u5e73\u5747\u8cfc\u8cb7\u91d1\u984d\u7d1a\u8ddd(\u81ea2008\u5e74) C019 \u624b\u6a5f\u6700\u8fd1\u8cfc\u8cb7\u91d1\u984d\u7d1a\u8ddd(\u81ea2008\u5e74) C020 \u901a\u4fe1\u7d81\u7d04\u6b21\u6578\n",
      "\n",
      "C021 \u8cfc\u6a5f\u7d81\u7d04\u6b21\u6578 C022 NP\u696d\u8005[\u7528\u6236] C023 \u5047NP\u72c0\u614b[\u7528\u6236] C024 NP\u6b21\u6578[\u7528\u6236] C025 NPIN\u4e2d\u83ef\u6b21\u6578[\u7528\u6236] C026 NPIN\u7af6\u696dA\u6b21\u6578[\u7528\u6236] C027 NPIN\u7af6\u696dB\u6b21\u6578[\u7528\u6236] C028 NPOUT\u4e2d\u83ef\u6b21\u6578[\u7528\u6236] C029 NPOUT\u7af6\u696dA\u6b21\u6578[\u7528\u6236] C030 NPOUT\u7af6\u696dB\u6b21\u6578[\u7528\u6236]\n",
      "\n",
      "C031 [\u884c\u52d5]\u5148\u524d\u662f\u5426\u70baNPIN C032 [\u884c\u52d5]\u6700\u8fd1\u7684NP\u6642\u9593 C033 [\u884c\u52d5]\u6700\u4e45\u7684NP\u6642\u9593 C034 [\u884c\u52d5]NP\u6b21\u6578 C035 [\u5ba2\u6236]\u7528\u6236\u8a2d\u5099\u7e3d\u6578 C036 [\u5ba2\u6236]\u884c\u52d5\u7528\u6236\u8a2d\u5099\u6578 C037 [\u5ba2\u6236]MOD\u7528\u6236\u8a2d\u5099\u6578 C038 [\u5ba2\u6236]\u5e02\u8a71\u7528\u6236\u8a2d\u5099\u6578 C039 [\u5ba2\u6236]\u56fa\u7db2\u975e\u5e02\u8a71\u7528\u6236\u8a2d\u5099\u6578 C040 [\u5ba2\u6236]\u5149\u4e16\u4ee3\u696d\u52d9\u7528\u6236\u8a2d\u5099\u6578\n",
      "\n",
      "C041 [\u5ba2\u6236]HiNet\u7528\u6236\u8a2d\u5099\u6578 C042 [\u5ba2\u6236]\u5176\u4ed6\u7528\u6236\u8a2d\u5099\u6578 C043 [\u5ba2\u6236]\u540c\u4e00\u5ba2\u6236\u6b78\u5c6c\u4e4b\u5bb6\u6236\u6578 C044 [\u7528\u6236]\u96fb\u5b50\u5e33\u55ae\u8b58\u5225 C045 [\u7528\u6236]\u4e0d\u5bc4\u9001\u5ee3\u544a\u8b58\u5225\u6b04 C046 [\u884c\u52d5]\u4e0d\u63a5\u53d7\u96fb\u8a71\u4fc3\u92b7\u8b58\u5225 C047 [\u884c\u52d5]\u6578\u64da\u7e3dMB\u91cf C048 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7e3d\u5206\u9418\u6578 C049 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7db2\u5167\u5206\u9418\u6578 C050 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u71b1\u7dda\u5206\u9418\u6578\n",
      "\n",
      "C051 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7db2\u5916\u5206\u9418\u6578 C052 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u5e02\u8a71\u5206\u9418\u6578 C053 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u570b\u969b\u5206\u9418\u6578 C054 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dA\u5206\u9418\u6578 C055 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dB\u5206\u9418\u6578 C056 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dC\u5206\u9418\u6578 C057 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dD\u5206\u9418\u6578 C058 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7db2\u5167\u5206\u9418\u6578\u6bd4\u4f8b C059 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u71b1\u7dda\u5206\u9418\u6578\u6bd4\u4f8b C060 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7db2\u5916\u5206\u9418\u6578\u6bd4\u4f8b\n",
      "\n",
      "C061 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u5e02\u8a71\u5206\u9418\u6578\u6bd4\u4f8b C062 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u570b\u969b\u5206\u9418\u6578\u6bd4\u4f8b C063 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u52a0\u503c\u5206\u9418\u6578\u6bd4\u4f8b C064 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dA\u5206\u9418\u6578\u6bd4\u4f8b C065 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dB\u5206\u9418\u6578\u6bd4\u4f8b C066 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dC\u5206\u9418\u6578\u6bd4\u4f8b C067 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dD\u5206\u9418\u6578\u6bd4\u4f8b C068 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb500_02 C069 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb502_04 C070 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb504_06\n",
      "\n",
      "C071 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb506_08 C072 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb508_10 C073 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb510_12 C074 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb512_14 C075 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb514_16 C076 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb516_18 C077 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb518_20 C078 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb520_22 C079 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb522_24 C080 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb500_02\u6bd4\u4f8b\n",
      "\n",
      "C081 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb502_04\u6bd4\u4f8b C082 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb504_06\u6bd4\u4f8b C083 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb506_08\u6bd4\u4f8b C084 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb508_10\u6bd4\u4f8b C085 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb510_12\u6bd4\u4f8b C086 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb512_14\u6bd4\u4f8b C087 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb514_16\u6bd4\u4f8b C088 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb516_18\u6bd4\u4f8b C089 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb518_20\u6bd4\u4f8b C090 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb520_22\u6bd4\u4f8b\n",
      "\n",
      "C091 [\u884c\u52d5]\u8a9e\u97f3\u5e73\u5747\u767c\u8a71\u5206\u9418\u6578\u6642\u6bb522_24\u6bd4\u4f8b C092 [\u884c\u52d5]\u5c45\u4f4f\u5730\u57ce\u5e02 C093 [\u884c\u52d5]\u5c45\u4f4f\u5730\u884c\u653f\u5340 C094 [\u884c\u52d5]\u5de5\u4f5c\u5730\u57ce\u5e02 C095 [\u884c\u52d5]\u5de5\u4f5c\u5730\u884c\u653f\u5340 C096 [\u884c\u52d5]\u66f4\u63db\u5c45\u4f4f\u5730 C097 [\u884c\u52d5]\u66f4\u63db\u4e0a\u73ed\u5730 C098 [\u884c\u52d5]\u5e73\u65e5\u5916\u5bbf\u5929\u6578 C099 [\u884c\u52d5]\u9031\u672b\u5916\u5bbf\u5929\u6578 C100 [\u884c\u52d5]\u5e73\u65e5\u5c45\u5bb6\u6642\u9593\n",
      "\n",
      "C101 [\u884c\u52d5]\u5047\u65e5\u5c45\u5bb6\u6642\u9593 C102 [\u884c\u52d5]\u5e73\u65e5\u79fb\u52d5\u8ddd\u96e2sumKM C103 [\u884c\u52d5]\u9031\u672b\u79fb\u52d5\u8ddd\u96e2sumKM C104 [\u884c\u52d5]\u5e73\u65e5\u79fb\u52d5\u6700\u9060\u8ddd\u96e2maxKM C105 [\u884c\u52d5]\u9031\u672b\u79fb\u52d5\u6700\u9060\u8ddd\u96e2maxKM C106 [\u884c\u52d5]\u901a\u52e4\u8ddd\u96e2 C107 [\u884c\u52d5]\u79d1\u6280\u5712\u5340 C108 [\u884c\u52d5]\u5047\u65e5\u642d\u4e58\u9ad8\u9435\u6b21\u6578 C109 [\u884c\u52d5]\u5e73\u65e5\u642d\u4e58\u9ad8\u9435\u6b21\u6578 C110 [\u884c\u52d5]\u5728\u570b\u5916\u7684\u5929\u6578\n",
      "\n",
      "C111 [\u884c\u52d5]\u4f4d\u65bc\u767e\u8ca8\u516c\u53f8\u7684\u5929\u6578 C112 [\u884c\u52d5]\u4f4d\u65bc\u8cfc\u7269\u4e2d\u5fc3\u7684\u5929\u6578 C113 [\u884c\u52d5]\u4f4d\u65bc\u751f\u6d3b\u91cf\u8ca9\u7684\u5929\u6578 C114 [\u884c\u52d5]\u4f4d\u65bc\u50a2\u98fe\u91cf\u8ca9\u7684\u5929\u6578 C115 [\u884c\u52d5]\u4f4d\u65bc\u65c5\u904a\u666f\u9ede\u7684\u5929\u6578 C116 [\u884c\u52d5]\u4f4d\u65bc\u96fb\u5f71\u9662\u7684\u5929\u6578 C117 [\u884c\u52d5]\u4f4d\u65bc\u5c55\u6f14\u7684\u5929\u6578 C118 [\u884c\u52d5]\u4f4d\u65bc\u9ad4\u80b2\u9928/\u5834\u7684\u5929\u6578 C119 [\u884c\u52d5]\u4f4d\u65bc\u706b\u8eca/\u9ad8\u9435\u7ad9\u7684\u5929\u6578 C120 [\u884c\u52d5]\u4f4d\u65bc\u91ab\u7642\u4fdd\u5065\u7684\u5929\u6578\n",
      "\n",
      "C121 [\u884c\u52d5]\u4f4d\u65bc\u6baf\u846c\u7684\u5929\u6578 C122 [\u884c\u52d5]\u4f4d\u65bc\u9ad8\u723e\u592b\u7403\u5834\u7684\u5929\u6578 C123 [\u884c\u52d5]\u5047\u65e5\u5728\u5bb6\u884c\u52d5\u4e0a\u7db2\u91cf(0:00~6:00) C124 [\u884c\u52d5]\u5047\u65e5\u5728\u5bb6\u884c\u52d5\u4e0a\u7db2\u91cf(6:00~12:00) C125 [\u884c\u52d5]\u5047\u65e5\u5728\u5bb6\u884c\u52d5\u4e0a\u7db2\u91cf(12:00~18:00) C126 [\u884c\u52d5]\u5047\u65e5\u5728\u5bb6\u884c\u52d5\u4e0a\u7db2\u91cf(18:00~24:00) C127 [\u884c\u52d5]\u5047\u65e5\u5728\u5916\u884c\u52d5\u4e0a\u7db2\u91cf(0:00~6:00) C128 [\u884c\u52d5]\u5047\u65e5\u5728\u5916\u884c\u52d5\u4e0a\u7db2\u91cf(6:00~12:00) C129 [\u884c\u52d5]\u5047\u65e5\u5728\u5916\u884c\u52d5\u4e0a\u7db2\u91cf(12:00~18:00) C130 [\u884c\u52d5]\u5047\u65e5\u5728\u5916\u884c\u52d5\u4e0a\u7db2\u91cf(18:00~24:00)\n",
      "\n",
      "C131 [\u884c\u52d5]\u5047\u65e5\u5728\u8fa6\u516c\u5730\u884c\u52d5\u4e0a\u7db2\u91cf(0:00~6:00) C132 [\u884c\u52d5]\u5047\u65e5\u5728\u8fa6\u516c\u5730\u884c\u52d5\u4e0a\u7db2\u91cf(6:00~12:00) C133 [\u884c\u52d5]\u5047\u65e5\u5728\u8fa6\u516c\u5730\u884c\u52d5\u4e0a\u7db2\u91cf(12:00~18:00) C134 [\u884c\u52d5]\u5047\u65e5\u5728\u8fa6\u516c\u5730\u884c\u52d5\u4e0a\u7db2\u91cf(18:00~24:00) C135 [\u884c\u52d5]\u5e73\u65e5\u5728\u5bb6\u884c\u52d5\u4e0a\u7db2\u91cf(0:00~6:00) C136 [\u884c\u52d5]\u5e73\u65e5\u5728\u5bb6\u884c\u52d5\u4e0a\u7db2\u91cf(6:00~12:00) C137 [\u884c\u52d5]\u5e73\u65e5\u5728\u5bb6\u884c\u52d5\u4e0a\u7db2\u91cf(12:00~18:00) C138 [\u884c\u52d5]\u5e73\u65e5\u5728\u5bb6\u884c\u52d5\u4e0a\u7db2\u91cf(18:00~24:00) C139 [\u884c\u52d5]\u5e73\u65e5\u5728\u5916\u884c\u52d5\u4e0a\u7db2\u91cf(0:00~6:00) C140 [\u884c\u52d5]\u5e73\u65e5\u5728\u5916\u884c\u52d5\u4e0a\u7db2\u91cf(6:00~12:00)\n",
      "\n",
      "C141 [\u884c\u52d5]\u5e73\u65e5\u5728\u5916\u884c\u52d5\u4e0a\u7db2\u91cf(12:00~18:00) C142 [\u884c\u52d5]\u5e73\u65e5\u5728\u5916\u884c\u52d5\u4e0a\u7db2\u91cf(18:00~24:00) C143 [\u884c\u52d5]\u5e73\u65e5\u5728\u8fa6\u516c\u5730\u884c\u52d5\u4e0a\u7db2\u91cf(0:00~6:00) C144 [\u884c\u52d5]\u5e73\u65e5\u5728\u8fa6\u516c\u5730\u884c\u52d5\u4e0a\u7db2\u91cf(6:00~12:00) C145 [\u884c\u52d5]\u5e73\u65e5\u5728\u8fa6\u516c\u5730\u884c\u52d5\u4e0a\u7db2\u91cf(12:00~18:00) C146 [\u884c\u52d5]\u5e73\u65e5\u5728\u8fa6\u516c\u5730\u884c\u52d5\u4e0a\u7db2\u91cf(18:00~24:00) C147 [\u884c\u52d5]\u8a9e\u97f3\u901a\u8a71\u5206\u9418\u6578 C148 [\u884c\u52d5]\u4e2d\u83ef\u884c\u52d5\u901a\u8a71\u5206\u9418\u6578 C149 [\u884c\u52d5]\u975e\u4e2d\u83ef\u884c\u52d5\u901a\u8a71\u5206\u9418\u6578 C150 [\u884c\u52d5]\u5e02\u8a71\u901a\u8a71\u5206\u9418\u6578\n",
      "\n",
      "C151 [\u884c\u52d5]\u4e2d\u83ef\u5e02\u8a71\u901a\u8a71\u5206\u9418\u6578 C152 [\u884c\u52d5]\u975e\u4e2d\u83ef\u5e02\u8a71\u901a\u8a71\u5206\u9418\u6578 C153 [\u884c\u52d5]\u570b\u969b\u901a\u8a71\u5206\u9418\u6578 C154 [\u884c\u52d5]\u6700\u5e38\u767c\u8a71\u884c\u52d5\u96fb\u4fe1\u696d\u8005\u5206\u9418\u6578 C155 [\u884c\u52d5]\u8a9e\u97f3\u901a\u8a71\u901a\u6578 C156 [\u884c\u52d5]\u4e2d\u83ef\u884c\u52d5\u901a\u8a71\u901a\u6578 C157 [\u884c\u52d5]\u975e\u4e2d\u83ef\u884c\u52d5\u901a\u8a71\u901a\u6578 C158 [\u884c\u52d5]\u5e02\u8a71\u901a\u8a71\u901a\u6578 C159 [\u884c\u52d5]\u4e2d\u83ef\u5e02\u8a71\u901a\u8a71\u901a\u6578 C160 [\u884c\u52d5]\u975e\u4e2d\u83ef\u5e02\u8a71\u901a\u8a71\u901a\u6578\n",
      "\n",
      "C161 [\u884c\u52d5]\u570b\u969b\u901a\u8a71\u901a\u6578 C162 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7e3d\u5c0d\u8c61\u6578 C163 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7db2\u5167\u5c0d\u8c61\u6578 C164 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u71b1\u7dda\u5c0d\u8c61\u6578 C165 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7db2\u5916\u5c0d\u8c61\u6578 C166 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u5e02\u8a71\u5c0d\u8c61\u6578 C167 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u570b\u969b\u5c0d\u8c61\u6578 C168 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dA\u5c0d\u8c61\u6578 C169 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dB\u5c0d\u8c61\u6578 C170 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dC\u5c0d\u8c61\u6578\n",
      "\n",
      "C171 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dD\u5c0d\u8c61\u6578 C172 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7db2\u5167\u5c0d\u8c61\u6578\u6bd4\u4f8b C173 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u71b1\u7dda\u5c0d\u8c61\u6578\u6bd4\u4f8b C174 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7db2\u5916\u5c0d\u8c61\u6578\u6bd4\u4f8b C175 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u5e02\u8a71\u5c0d\u8c61\u6578\u6bd4\u4f8b C176 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u570b\u969b\u5c0d\u8c61\u6578\u6bd4\u4f8b C177 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u52a0\u503c\u5c0d\u8c61\u6578\u6bd4\u4f8b C178 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dA\u5c0d\u8c61\u6578\u6bd4\u4f8b C179 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dB\u5c0d\u8c61\u6578\u6bd4\u4f8b C180 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dC\u5c0d\u8c61\u6578\u6bd4\u4f8b\n",
      "\n",
      "C181 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dD\u5c0d\u8c61\u6578\u6bd4\u4f8b C182 [\u884c\u52d5]\u767c\u8a71\u5c0d\u8c61\u6578 C183 [\u884c\u52d5]\u4e2d\u83ef\u884c\u52d5\u767c\u8a71\u5c0d\u8c61\u6578 C184 [\u884c\u52d5]\u975e\u4e2d\u83ef\u884c\u52d5\u767c\u8a71\u5c0d\u8c61\u6578 C185 [\u884c\u52d5]\u5e02\u8a71\u767c\u8a71\u5c0d\u8c61\u6578\n",
      "C186 [\u884c\u52d5]\u570b\u969b\u767c\u8a71\u5c0d\u8c61\u6578 C187 [\u884c\u52d5]\u8a9e\u97f3\u53d7\u8a71\u7e3d\u5206\u9418\u6578 C188 [\u884c\u52d5]\u8a9e\u97f3\u53d7\u8a71\u7db2\u5167\u5206\u9418\u6578 C189 [\u884c\u52d5]\u8a9e\u97f3\u53d7\u8a71\u7db2\u5916\u5206\u9418\u6578 C190 [\u884c\u52d5]\u8a9e\u97f3\u53d7\u8a71\u5e02\u8a71\u5206\u9418\u6578\n",
      "\n",
      "C191 [\u884c\u52d5]\u8a9e\u97f3\u53d7\u8a71\u7db2\u5167\u5206\u9418\u6578\u6bd4\u4f8b C192 [\u884c\u52d5]\u8a9e\u97f3\u53d7\u8a71\u7db2\u5916\u5206\u9418\u6578\u6bd4\u4f8b C193 [\u884c\u52d5]\u8a9e\u97f3\u53d7\u8a71\u5e02\u8a71\u5206\u9418\u6578\u6bd4\u4f8b C194 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7e3d\u6b21\u6578 C195 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7db2\u5167\u6b21\u6578 C196 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u71b1\u7dda\u6b21\u6578 C197 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7db2\u5916\u6b21\u6578 C198 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u5e02\u8a71\u6b21\u6578 C199 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u570b\u969b\u6b21\u6578 C200 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dA\u6b21\u6578\n",
      "\n",
      "C201 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dB\u6b21\u6578 C202 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dC\u6b21\u6578 C203 [\u884c\u52d5]\u8a9e\u97f3\u767c\u8a71\u7af6\u696dD\u6b21\u6578 C204 [\u884c\u52d5]\u8a9e\u97f3\u53d7\u8a71\u7e3d\u6b21\u6578 C205 [\u884c\u52d5]\u8a9e\u97f3\u53d7\u8a71\u7db2\u5167\u6b21\u6578 C206 [\u884c\u52d5]\u8a9e\u97f3\u53d7\u8a71\u7db2\u5916\u6b21\u6578 C207 [\u884c\u52d5]\u8a9e\u97f3\u53d7\u8a71\u5e02\u8a71\u6b21\u6578 C208 [\u884c\u52d5]\u6700\u5e38\u767c\u8a71\u96fb\u4fe1\u696d\u8005 C209 [\u884c\u52d5]\u66fe\u7d93\u767c\u8a71\u57fa\u7ad9\u6578 C210 [\u884c\u52d5]\u6279\u50f9\u7c21\u8a0a\u6b21\u6578\n",
      "\n",
      "C211 [\u884c\u52d5]\u6279\u50f9\u6578\u64da\u9023\u7dda\u6b21\u6578 C212 [\u884c\u52d5]\u6578\u64da\u4e0a\u50b3MB C213 [\u884c\u52d5]\u6578\u64da\u4e0a\u50b3MB\u6bd4\u4f8b C214 [\u884c\u52d5]\u884c\u52d5\u6578\u64da\u6b21\u6578 C215 [\u884c\u52d5]\u884c\u52d5\u6578\u64da\u6d41\u91cf"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.to_csv('../../df_676.txt', encoding='utf-8', sep='\\t')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    }
   ],
   "metadata": {}
  }
 ]
}